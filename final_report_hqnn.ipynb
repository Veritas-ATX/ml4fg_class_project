{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263a3272-e8cd-4b51-9aaf-932ce45fb06d",
   "metadata": {},
   "source": [
    "# ML4FG Final Report HQNN Code\n",
    "### By: Austin Stiefelmaier 12/14/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f775c36-6b70-41d0-8e66-e17f5b94e34f",
   "metadata": {},
   "source": [
    "## Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by quantum ML code from:\n",
    "# https://pennylane.ai/qml/demos/tutorial_quantum_transfer_learning.html\n",
    "# And classical ML code from:\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbadb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Pennylane imports\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcb8f46-b67f-4c99-afdf-0a6a980f1dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Fix random num generation for reproducibility \n",
    "torch.manual_seed(7)\n",
    "np.random.seed(7)\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# If GPU available, set to run on it\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94266a11-f651-4e30-9fb5-885f6715ffe7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d566b0e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Data download\n",
    "df_x = pd.read_csv('/home/as6734/ml4fg_class_project/TCGA-PANCAN-HiSeq-801x20531/data.csv')\n",
    "df_y = pd.read_csv('/home/as6734/ml4fg_class_project/TCGA-PANCAN-HiSeq-801x20531/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c85d30-ced3-476c-9121-29c57dbe33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "\n",
    "# Drop first column that only notes sample number (which can be reconstructed from index if need be\n",
    "df_x.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_y.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "# Remove columns with all 0.0 values\n",
    "df_x = df_x.loc[:, (df_x != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dc4c81-aa50-41a8-a112-e0b8e69d2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "\n",
    "# Normalize values by mean\n",
    "df_x=(df_x-df_x.mean())/df_x.std()\n",
    "# Encode classes\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(df_y.values)\n",
    "y = enc.transform(df_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92ec11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 512, 'validation': 128, 'test': 161}\n"
     ]
    }
   ],
   "source": [
    "# Split data, final percentages are approximately 64% train, 16% validation, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x.values, y, test_size=0.2, train_size=0.8, random_state=7, stratify=y)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, train_size=0.8, random_state=7, stratify=y_train)\n",
    "dataset_sizes = {'train': len(x_train), 'validation': len(x_valid), 'test': len(x_test)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f091b5d5-768f-4909-a471-8e31bfe3656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data reductions to match percentages noted in future cells, \n",
    "# NOTE: all reduced slightly to %8\n",
    "x_train = x_train[0:int(dataset_sizes['train']*0.8-(dataset_sizes['train']*0.8)%8)]\n",
    "y_train = y_train[0:int(dataset_sizes['train']*0.8-(dataset_sizes['train']*0.8)%8)]\n",
    "\n",
    "# x_train = x_train[0:int(dataset_sizes['train']*0.6-(dataset_sizes['train']*0.6)%8)]\n",
    "# y_train = y_train[0:int(dataset_sizes['train']*0.6-(dataset_sizes['train']*0.6)%8)]\n",
    "\n",
    "# x_train = x_train[0:int(dataset_sizes['train']*0.4-(dataset_sizes['train']*0.4)%8)]\n",
    "# y_train = y_train[0:int(dataset_sizes['train']*0.4-(dataset_sizes['train']*0.4)%8)]\n",
    "\n",
    "# x_train = x_train[0:int(dataset_sizes['train']*0.2-(dataset_sizes['train']*0.2)%8)]\n",
    "# y_train = y_train[0:int(dataset_sizes['train']*0.2-(dataset_sizes['train']*0.2)%8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22283318-279b-4514-8a88-dfc8ef9f47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoaders\n",
    "x_train_to_tensor = torch.from_numpy(x_train).to(torch.float32).to(device)\n",
    "y_train_to_tensor = torch.from_numpy(y_train).to(torch.float32).to(device)\n",
    "x_valid_to_tensor = torch.from_numpy(x_valid).to(torch.float32).to(device)\n",
    "y_valid_to_tensor = torch.from_numpy(y_valid).to(torch.float32).to(device)\n",
    "x_test_to_tensor = torch.from_numpy(x_test).to(torch.float32).to(device)\n",
    "y_test_to_tensor = torch.from_numpy(y_test).to(torch.float32).to(device)\n",
    "\n",
    "# Second step: Creating TensorDataset for Dataloader\n",
    "train_set = TensorDataset(x_train_to_tensor, y_train_to_tensor)\n",
    "valid_set = TensorDataset(x_valid_to_tensor, y_valid_to_tensor)\n",
    "test_set = TensorDataset(x_test_to_tensor, y_test_to_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True),\n",
    "    'validation': torch.utils.data.DataLoader(valid_set, batch_size=8),\n",
    "    'test': torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588255e8-f029-4b48-88b5-6591ac9d4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data downsampling helper function\n",
    "# def downsample_train(x, y, percentage):\n",
    "#     # Get number of datapoints to use in training set\n",
    "#     total = round(dataset_sizes['train'] * (percentage / 100))\n",
    "#     # Deepcopy\n",
    "#     x_train = np.copy(x)\n",
    "#     y_train = np.copy(y)\n",
    "#     # Reduce samples\n",
    "#     x_train = x_train[0:total]\n",
    "#     y_train = y_train[0:total]\n",
    "#     x_train_to_tensor = torch.from_numpy(x_train).to(torch.float32).to(device)\n",
    "#     y_train_to_tensor = torch.from_numpy(y_train).to(torch.float32).to(device)\n",
    "#     train_set = TensorDataset(x_train_to_tensor, y_train_to_tensor)\n",
    "#     return torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40e428",
   "metadata": {},
   "source": [
    "## Quantum Circuit Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b4fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum circuit params\n",
    "qubit_count = 5\n",
    "circ_repeats = 6  # How many times to repeat RY and CNOT gates\n",
    "q_delta = 0.01  # Param for quantum circuit weight initialization\n",
    "\n",
    "# Run quantum circuit on Pennylane default simulator\n",
    "dev = qml.device('default.qubit', wires=qubit_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec083a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to construct quantum circuit\n",
    "def ry_gates(w):\n",
    "    # Apply rotation gate RY w/ given weights\n",
    "    for i, weight in enumerate(w):\n",
    "        qml.RY(weight, wires=i)\n",
    "\n",
    "# Adds alternating CNOT layer to entangle qubits\n",
    "def entangling_layer(nqubits):\n",
    "    for i in range(0, nqubits - 1, 2):  # Evens\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Odds\n",
    "        qml.CNOT(wires=[i, i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f695c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct quantum circuit to plug into PyTorch\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def quantum_circuit(inputs, q_weights_flat):\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(circ_repeats, qubit_count)\n",
    "    # Initialize w/ H gates so orthogonal to computational basis\n",
    "    # This helps start w/out bias towards 0 or 1 states\n",
    "    for i in range(qubit_count):\n",
    "        qml.Hadamard(wires=i)\n",
    "    # Take given inputs and apply to quantum circuit as first weights\n",
    "    ry_gates(inputs)\n",
    "    # Repeat CNOT and RY gates to add more weights to train and\n",
    "    # CNOT \"convolutions\" (really entangles)\n",
    "    for k in range(circ_repeats):\n",
    "        entangling_layer(qubit_count)\n",
    "        ry_gates(q_weights[k])\n",
    "    # Use Pennylane sim to get expected value after applying Z gate\n",
    "    # which returns to standard computational basis, this is layer output\n",
    "    expected_vals = [qml.expval(qml.PauliZ(entry)) for entry in range(qubit_count)]\n",
    "    return tuple(expected_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293270e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──H──RY(-0.15)─╭●──RY(-0.01)────────────╭●──RY(0.00)────────────╭●──RY(-0.01)────────────╭●\n",
      "1: ──H──RY(0.69)──╰X─╭●──────────RY(0.00)──╰X─╭●─────────RY(-0.01)─╰X─╭●──────────RY(-0.00)─╰X\n",
      "2: ──H──RY(0.21)──╭●─╰X──────────RY(0.01)──╭●─╰X─────────RY(-0.01)─╭●─╰X──────────RY(0.01)──╭●\n",
      "3: ──H──RY(-0.84)─╰X─╭●──────────RY(-0.01)─╰X─╭●─────────RY(0.01)──╰X─╭●──────────RY(0.01)──╰X\n",
      "4: ──H──RY(-0.98)────╰X──────────RY(-0.00)────╰X─────────RY(-0.01)────╰X──────────RY(-0.01)───\n",
      "\n",
      "───RY(0.01)────────────╭●──RY(0.01)────────────╭●──RY(0.01)────────────┤  <Z>\n",
      "──╭●─────────RY(-0.00)─╰X─╭●─────────RY(-0.01)─╰X─╭●─────────RY(0.02)──┤  <Z>\n",
      "──╰X─────────RY(-0.00)─╭●─╰X─────────RY(0.01)──╭●─╰X─────────RY(-0.01)─┤  <Z>\n",
      "──╭●─────────RY(0.00)──╰X─╭●─────────RY(0.01)──╰X─╭●─────────RY(-0.00)─┤  <Z>\n",
      "──╰X─────────RY(0.02)─────╰X─────────RY(-0.01)────╰X─────────RY(-0.00)─┤  <Z>\n"
     ]
    }
   ],
   "source": [
    "# Example visualization codes with nonsense weights\n",
    "# Helps to see what the quantum circuit looks like\n",
    "q_params_print = nn.Parameter(q_delta * torch.randn(circ_repeats * qubit_count))\n",
    "input_test = torch.randn(1,512)\n",
    "pre_net_test = nn.Linear(512, qubit_count)\n",
    "pre_out_test = pre_net_test(input_test)\n",
    "q_in_test = torch.tanh(pre_out_test) * np.pi / 2.0\n",
    "for elem in q_in_test:\n",
    "    print(qml.draw(quantum_circuit)(elem, q_params_print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916720a3-5e54-4bee-b428-7be9e3252bdc",
   "metadata": {},
   "source": [
    "## HQNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c56560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HQNN Model Class\n",
    "class HybridQuantumNet(nn.Module):\n",
    "    # Initialize layers\n",
    "    def __init__(self, num_feature):\n",
    "        super().__init__()\n",
    "        # Layers before quantum circuit\n",
    "        self.layer_1 = nn.Linear(num_feature, 512)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.layer_2 = nn.Linear(512, 128)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.layer_3 = nn.Linear(128, 64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.layer_4 = nn.Linear(64, qubit_count)\n",
    "        # Quantum Circuit params\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(circ_repeats * qubit_count))\n",
    "        # Layer(s) after quantum circuit\n",
    "        # self.post_quant = torch.nn.Softmax(dim=qubit_count)\n",
    "        # self.post_quant = nn.Linear(qubit_count, 5)\n",
    "        # Misc\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    # Forward pass procedure\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third layer\n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Map classical 64 output to qubit_count output\n",
    "        pre_out = self.layer_4(x)\n",
    "        \n",
    "        # Convert to radians for use as RY rotation gate params\n",
    "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
    "        # Apply quantum circuit set to run on GPU\n",
    "        q_out = torch.Tensor(0, qubit_count)\n",
    "        q_out = q_out.to(device)\n",
    "        for elem in q_in:\n",
    "            q_out_elem = quantum_circuit(elem, self.q_params)\n",
    "            q_out_elem = torch.stack(list(q_out_elem), dim=0)\n",
    "            q_out_elem = q_out_elem.float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "        # Map quantum circuit output using softmax to classes\n",
    "        # return self.post_quant(q_out)\n",
    "        return q_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "036f4bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridQuantumNet(\n",
      "  (layer_1): Linear(in_features=20264, out_features=512, bias=True)\n",
      "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_4): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model_hybrid = HybridQuantumNet(num_feature=len(df_x.columns))\n",
    "# Make sure set to use GPU\n",
    "model_hybrid = model_hybrid.to(device)\n",
    "# Show model architecture summary\n",
    "print(model_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ef09c-cd06-41b8-bd44-35bded59fab8",
   "metadata": {},
   "source": [
    "## Train HQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad1b52f9-0e31-4ea4-8edb-41272c82a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.0004  # Initial learning rate\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "gamma_lr_scheduler = 0.1  # Learning rate decay param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24238e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss, optimizer, and learning rate decay manager\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer_hybrid = optim.Adam(model_hybrid.parameters(), lr=step)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "    optimizer_hybrid, step_size=10, gamma=gamma_lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "035d2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model(s)\n",
    "def train(model, loss_func, optimizer, scheduler, num_epochs):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100000.0\n",
    "    best_loss_train = 100000.0\n",
    "    print('Training started:')\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                # Set model to train mode\n",
    "                model.train()\n",
    "            else:\n",
    "                # Set model to eval mode\n",
    "                model.eval()\n",
    "            current_loss = 0.0\n",
    "            n_batches = dataset_sizes[phase] // batch_size\n",
    "            iter = 0\n",
    "            for X, Y in dataloaders[phase]:\n",
    "                batch_len = len(X)\n",
    "                X = X.to(device)\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                # If in train mode, get loss, step optimizer\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(X)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = loss_func(outputs, Y)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # Print iteration results\n",
    "                current_loss += loss.item() * batch_len\n",
    "                print('Phase: {} Epoch: {}/{} Iter: {}/{}'.format(phase, epoch+1, num_epochs, iter+1, n_batches+1),\n",
    "                    end=\"\\r\",\n",
    "                    flush=True)\n",
    "                iter += 1\n",
    "\n",
    "            # Get epoch stats and print\n",
    "            epoch_loss = current_loss / dataset_sizes[phase]\n",
    "            print('Phase: {} Epoch: {}/{} Loss: {:.4f}        '.format(\n",
    "                    'train' if phase == 'train' else 'validation  ',\n",
    "                    epoch + 1,\n",
    "                    num_epochs,\n",
    "                    epoch_loss,\n",
    "                )\n",
    "            )\n",
    "            # Update best vars and make copy of best weights\n",
    "            # if phase == \"validation\":\n",
    "            #     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"validation\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
    "                best_loss_train = epoch_loss\n",
    "            # Decay learning rate\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "    # Print final results\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Best test loss: {:.4f}'.format(best_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efff214c-35cd-4a9c-8564-6388316353d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd33bb83b2a455d94139a7258a485be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train Epoch: 1/50 Loss: 0.9154        \n",
      "Phase: validation   Epoch: 1/50 Loss: 0.7488        \n",
      "Phase: train Epoch: 2/50 Loss: 0.6526        \n",
      "Phase: validation   Epoch: 2/50 Loss: 0.6211        \n",
      "Phase: train Epoch: 3/50 Loss: 0.5696        \n",
      "Phase: validation   Epoch: 3/50 Loss: 0.5339        \n",
      "Phase: train Epoch: 4/50 Loss: 0.5389        \n",
      "Phase: validation   Epoch: 4/50 Loss: 0.4921        \n",
      "Phase: train Epoch: 5/50 Loss: 0.4797        \n",
      "Phase: validation   Epoch: 5/50 Loss: 0.4730        \n",
      "Phase: train Epoch: 6/50 Loss: 0.4450        \n",
      "Phase: validation   Epoch: 6/50 Loss: 0.4585        \n",
      "Phase: train Epoch: 7/50 Loss: 0.4584        \n",
      "Phase: validation   Epoch: 7/50 Loss: 0.4635        \n",
      "Phase: train Epoch: 8/50 Loss: 0.4222        \n",
      "Phase: validation   Epoch: 8/50 Loss: 0.4525        \n",
      "Phase: train Epoch: 9/50 Loss: 0.4095        \n",
      "Phase: validation   Epoch: 9/50 Loss: 0.4437        \n",
      "Phase: train Epoch: 10/50 Loss: 0.4211        \n",
      "Phase: validation   Epoch: 10/50 Loss: 0.4472        \n",
      "Phase: train Epoch: 11/50 Loss: 0.3958        \n",
      "Phase: validation   Epoch: 11/50 Loss: 0.4512        \n",
      "Phase: train Epoch: 12/50 Loss: 0.3988        \n",
      "Phase: validation   Epoch: 12/50 Loss: 0.4441        \n",
      "Phase: train Epoch: 13/50 Loss: 0.4217        \n",
      "Phase: validation   Epoch: 13/50 Loss: 0.4437        \n",
      "Phase: train Epoch: 14/50 Loss: 0.4135        \n",
      "Phase: validation   Epoch: 14/50 Loss: 0.4452        \n",
      "Phase: train Epoch: 15/50 Loss: 0.3927        \n",
      "Phase: validation   Epoch: 15/50 Loss: 0.4432        \n",
      "Phase: train Epoch: 16/50 Loss: 0.3864        \n",
      "Phase: validation   Epoch: 16/50 Loss: 0.4451        \n",
      "Phase: train Epoch: 17/50 Loss: 0.3909        \n",
      "Phase: validation   Epoch: 17/50 Loss: 0.4413        \n",
      "Phase: train Epoch: 18/50 Loss: 0.3881        \n",
      "Phase: validation   Epoch: 18/50 Loss: 0.4445        \n",
      "Phase: train Epoch: 19/50 Loss: 0.4026        \n",
      "Phase: validation   Epoch: 19/50 Loss: 0.4428        \n",
      "Phase: train Epoch: 20/50 Loss: 0.4032        \n",
      "Phase: validation   Epoch: 20/50 Loss: 0.4437        \n",
      "Phase: train Epoch: 21/50 Loss: 0.3819        \n",
      "Phase: validation   Epoch: 21/50 Loss: 0.4433        \n",
      "Phase: train Epoch: 22/50 Loss: 0.3737        \n",
      "Phase: validation   Epoch: 22/50 Loss: 0.4412        \n",
      "Phase: train Epoch: 23/50 Loss: 0.4090        \n",
      "Phase: validation   Epoch: 23/50 Loss: 0.4432        \n",
      "Phase: train Epoch: 24/50 Loss: 0.3844        \n",
      "Phase: validation   Epoch: 24/50 Loss: 0.4436        \n",
      "Phase: train Epoch: 25/50 Loss: 0.3778        \n",
      "Phase: validation   Epoch: 25/50 Loss: 0.4463        \n",
      "Phase: train Epoch: 26/50 Loss: 0.3902        \n",
      "Phase: validation   Epoch: 26/50 Loss: 0.4445        \n",
      "Phase: train Epoch: 27/50 Loss: 0.4006        \n",
      "Phase: validation   Epoch: 27/50 Loss: 0.4437        \n",
      "Phase: train Epoch: 28/50 Loss: 0.3793        \n",
      "Phase: validation   Epoch: 28/50 Loss: 0.4430        \n",
      "Phase: train Epoch: 29/50 Loss: 0.3882        \n",
      "Phase: validation   Epoch: 29/50 Loss: 0.4449        \n",
      "Phase: train Epoch: 30/50 Loss: 0.3816        \n",
      "Phase: validation   Epoch: 30/50 Loss: 0.4434        \n",
      "Phase: train Epoch: 31/50 Loss: 0.3994        \n",
      "Phase: validation   Epoch: 31/50 Loss: 0.4398        \n",
      "Phase: train Epoch: 32/50 Loss: 0.3995        \n",
      "Phase: validation   Epoch: 32/50 Loss: 0.4406        \n",
      "Phase: train Epoch: 33/50 Loss: 0.3930        \n",
      "Phase: validation   Epoch: 33/50 Loss: 0.4413        \n",
      "Phase: train Epoch: 34/50 Loss: 0.3963        \n",
      "Phase: validation   Epoch: 34/50 Loss: 0.4419        \n",
      "Phase: train Epoch: 35/50 Loss: 0.3766        \n",
      "Phase: validation   Epoch: 35/50 Loss: 0.4413        \n",
      "Phase: train Epoch: 36/50 Loss: 0.3794        \n",
      "Phase: validation   Epoch: 36/50 Loss: 0.4449        \n",
      "Phase: train Epoch: 37/50 Loss: 0.3694        \n",
      "Phase: validation   Epoch: 37/50 Loss: 0.4413        \n",
      "Phase: train Epoch: 38/50 Loss: 0.3896        \n",
      "Phase: validation   Epoch: 38/50 Loss: 0.4446        \n",
      "Phase: train Epoch: 39/50 Loss: 0.3851        \n",
      "Phase: validation   Epoch: 39/50 Loss: 0.4417        \n",
      "Phase: train Epoch: 40/50 Loss: 0.3737        \n",
      "Phase: validation   Epoch: 40/50 Loss: 0.4411        \n",
      "Phase: train Epoch: 41/50 Loss: 0.3935        \n",
      "Phase: validation   Epoch: 41/50 Loss: 0.4419        \n",
      "Phase: train Epoch: 42/50 Loss: 0.3927        \n",
      "Phase: validation   Epoch: 42/50 Loss: 0.4463        \n",
      "Phase: train Epoch: 43/50 Loss: 0.4012        \n",
      "Phase: validation   Epoch: 43/50 Loss: 0.4416        \n",
      "Phase: train Epoch: 44/50 Loss: 0.4124        \n",
      "Phase: validation   Epoch: 44/50 Loss: 0.4509        \n",
      "Phase: train Epoch: 45/50 Loss: 0.3915        \n",
      "Phase: validation   Epoch: 45/50 Loss: 0.4433        \n",
      "Phase: train Epoch: 46/50 Loss: 0.3899        \n",
      "Phase: validation   Epoch: 46/50 Loss: 0.4436        \n",
      "Phase: train Epoch: 47/50 Loss: 0.3819        \n",
      "Phase: validation   Epoch: 47/50 Loss: 0.4429        \n",
      "Phase: train Epoch: 48/50 Loss: 0.3830        \n",
      "Phase: validation   Epoch: 48/50 Loss: 0.4416        \n",
      "Phase: train Epoch: 49/50 Loss: 0.3872        \n",
      "Phase: validation   Epoch: 49/50 Loss: 0.4431        \n",
      "Phase: train Epoch: 50/50 Loss: 0.3791        \n",
      "Phase: validation   Epoch: 50/50 Loss: 0.4438        \n",
      "Best test loss: 0.4398\n"
     ]
    }
   ],
   "source": [
    "model = train(model_hybrid, loss_func, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs)\n",
    "torch.save(model.state_dict(), './weights/updated60_hybrid_50epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e65fe04d-3a5f-4bc9-938d-a087bfd35232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a744ee9a7264dbfbb1d283a1852fd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       1.00      1.00      1.00        60\n",
      "        COAD       1.00      1.00      1.00        16\n",
      "        KIRC       1.00      1.00      1.00        30\n",
      "        LUAD       1.00      1.00      1.00        28\n",
      "        PRAD       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00       161\n",
      "   macro avg       1.00      1.00      1.00       161\n",
      "weighted avg       1.00      1.00      1.00       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds, targets = test(model_hybrid, 'cpu', dataloaders['test'])\n",
    "print(accuracy_score(y_true=targets, y_pred=preds))\n",
    "print(classification_report(y_true=targets, y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fd7721e-79bf-437d-9e03-e4371e35fd54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880c92ba7ed54ab09ef27b52b640bbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train Epoch: 1/50 Loss: 0.0088        \n",
      "Phase: validation   Epoch: 1/50 Loss: 0.7548        \n",
      "Phase: train Epoch: 2/50 Loss: 0.0093        \n",
      "Phase: validation   Epoch: 2/50 Loss: 0.7611        \n",
      "Phase: train Epoch: 3/50 Loss: 0.0096        \n",
      "Phase: validation   Epoch: 3/50 Loss: 0.7622        \n",
      "Phase: train Epoch: 4/50 Loss: 0.0083        \n",
      "Phase: validation   Epoch: 4/50 Loss: 0.7665        \n",
      "Phase: train Epoch: 5/50 Loss: 0.0080        \n",
      "Phase: validation   Epoch: 5/50 Loss: 0.7725        \n",
      "Phase: train Epoch: 6/50 Loss: 0.0089        \n",
      "Phase: validation   Epoch: 6/50 Loss: 0.7783        \n",
      "Phase: train Epoch: 7/50 Loss: 0.0084        \n",
      "Phase: validation   Epoch: 7/50 Loss: 0.7837        \n",
      "Phase: train Epoch: 8/50 Loss: 0.0071        \n",
      "Phase: validation   Epoch: 8/50 Loss: 0.7864        \n",
      "Phase: train Epoch: 9/50 Loss: 0.0083        \n",
      "Phase: validation   Epoch: 9/50 Loss: 0.7905        \n",
      "Phase: train Epoch: 10/50 Loss: 0.0075        \n",
      "Phase: validation   Epoch: 10/50 Loss: 0.7954        \n",
      "Phase: train Epoch: 11/50 Loss: 0.0072        \n",
      "Phase: validation   Epoch: 11/50 Loss: 0.8136        \n",
      "Phase: train Epoch: 12/50 Loss: 0.0073        \n",
      "Phase: validation   Epoch: 12/50 Loss: 0.8338        \n",
      "Phase: train Epoch: 13/50 Loss: 0.0069        \n",
      "Phase: validation   Epoch: 13/50 Loss: 0.8486        \n",
      "Phase: train Epoch: 14/50 Loss: 0.0070        \n",
      "Phase: validation   Epoch: 14/50 Loss: 0.8648        \n",
      "Phase: train Epoch: 15/50 Loss: 0.0076        \n",
      "Phase: validation   Epoch: 15/50 Loss: 0.8763        \n",
      "Phase: train Epoch: 16/50 Loss: 0.0075        \n",
      "Phase: validation   Epoch: 16/50 Loss: 0.8892        \n",
      "Phase: train Epoch: 17/50 Loss: 0.0074        \n",
      "Phase: validation   Epoch: 17/50 Loss: 0.8934        \n",
      "Phase: train Epoch: 18/50 Loss: 0.0070        \n",
      "Phase: validation   Epoch: 18/50 Loss: 0.9015        \n",
      "Phase: train Epoch: 19/50 Loss: 0.0078        \n",
      "Phase: validation   Epoch: 19/50 Loss: 0.9114        \n",
      "Phase: train Epoch: 20/50 Loss: 0.0069        \n",
      "Phase: validation   Epoch: 20/50 Loss: 0.9152        \n",
      "Phase: train Epoch: 21/50 Loss: 0.0082        \n",
      "Phase: validation   Epoch: 21/50 Loss: 0.9221        \n",
      "Phase: train Epoch: 22/50 Loss: 0.0074        \n",
      "Phase: validation   Epoch: 22/50 Loss: 0.9281        \n",
      "Phase: train Epoch: 23/50 Loss: 0.0074        \n",
      "Phase: validation   Epoch: 23/50 Loss: 0.9298        \n",
      "Phase: train Epoch: 24/50 Loss: 0.0078        \n",
      "Phase: validation   Epoch: 24/50 Loss: 0.9351        \n",
      "Phase: train Epoch: 25/50 Loss: 0.0073        \n",
      "Phase: validation   Epoch: 25/50 Loss: 0.9389        \n",
      "Phase: train Epoch: 26/50 Loss: 0.0075        \n",
      "Phase: validation   Epoch: 26/50 Loss: 0.9426        \n",
      "Phase: train Epoch: 27/50 Loss: 0.0068        \n",
      "Phase: validation   Epoch: 27/50 Loss: 0.9497        \n",
      "Phase: train Epoch: 28/50 Loss: 0.0070        \n",
      "Phase: validation   Epoch: 28/50 Loss: 0.9552        \n",
      "Phase: train Epoch: 29/50 Loss: 0.0074        \n",
      "Phase: validation   Epoch: 29/50 Loss: 0.9560        \n",
      "Phase: train Epoch: 30/50 Loss: 0.0075        \n",
      "Phase: validation   Epoch: 30/50 Loss: 0.9566        \n",
      "Phase: train Epoch: 31/50 Loss: 0.0068        \n",
      "Phase: validation   Epoch: 31/50 Loss: 0.9612        \n",
      "Phase: train Epoch: 32/50 Loss: 0.0084        \n",
      "Phase: validation   Epoch: 32/50 Loss: 0.9656        \n",
      "Phase: train Epoch: 33/50 Loss: 0.0071        \n",
      "Phase: validation   Epoch: 33/50 Loss: 0.9660        \n",
      "Phase: train Epoch: 34/50 Loss: 0.0070        \n",
      "Phase: validation   Epoch: 34/50 Loss: 0.9613        \n",
      "Phase: train Epoch: 35/50 Loss: 0.0081        \n",
      "Phase: validation   Epoch: 35/50 Loss: 0.9623        \n",
      "Phase: train Epoch: 36/50 Loss: 0.0073        \n",
      "Phase: validation   Epoch: 36/50 Loss: 0.9645        \n",
      "Phase: train Epoch: 37/50 Loss: 0.0070        \n",
      "Phase: validation   Epoch: 37/50 Loss: 0.9620        \n",
      "Phase: train Epoch: 38/50 Loss: 0.0074        \n",
      "Phase: validation   Epoch: 38/50 Loss: 0.9667        \n",
      "Phase: train Epoch: 39/50 Loss: 0.0074        \n",
      "Phase: validation   Epoch: 39/50 Loss: 0.9686        \n",
      "Phase: train Epoch: 40/50 Loss: 0.0067        \n",
      "Phase: validation   Epoch: 40/50 Loss: 0.9702        \n",
      "Phase: train Epoch: 41/50 Loss: 0.0070        \n",
      "Phase: validation   Epoch: 41/50 Loss: 0.9739        \n",
      "Phase: train Epoch: 42/50 Loss: 0.0074        \n",
      "Phase: validation   Epoch: 42/50 Loss: 0.9720        \n",
      "Phase: train Epoch: 43/50 Loss: 0.0067        \n",
      "Phase: validation   Epoch: 43/50 Loss: 0.9663        \n",
      "Phase: train Epoch: 44/50 Loss: 0.0068        \n",
      "Phase: validation   Epoch: 44/50 Loss: 0.9642        \n",
      "Phase: train Epoch: 45/50 Loss: 0.0072        \n",
      "Phase: validation   Epoch: 45/50 Loss: 0.9626        \n",
      "Phase: train Epoch: 46/50 Loss: 0.0070        \n",
      "Phase: validation   Epoch: 46/50 Loss: 0.9608        \n",
      "Phase: train Epoch: 47/50 Loss: 0.0072        \n",
      "Phase: validation   Epoch: 47/50 Loss: 0.9640        \n",
      "Phase: train Epoch: 48/50 Loss: 0.0080        \n",
      "Phase: validation   Epoch: 48/50 Loss: 0.9657        \n",
      "Phase: train Epoch: 49/50 Loss: 0.0065        \n",
      "Phase: validation   Epoch: 49/50 Loss: 0.9662        \n",
      "Phase: train Epoch: 50/50 Loss: 0.0070        \n",
      "Phase: validation   Epoch: 50/50 Loss: 0.9652        \n",
      "Best test loss: 0.7548\n",
      "Training started:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd64782ded34e8380a4841353645a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train Epoch: 1/50 Loss: 0.0404        \n",
      "Phase: validation   Epoch: 1/50 Loss: 0.8245        \n",
      "Phase: train Epoch: 2/50 Loss: 0.0431        \n",
      "Phase: validation   Epoch: 2/50 Loss: 0.8487        \n",
      "Phase: train Epoch: 3/50 Loss: 0.0399        \n",
      "Phase: validation   Epoch: 3/50 Loss: 0.8595        \n",
      "Phase: train Epoch: 4/50 Loss: 0.0376        \n",
      "Phase: validation   Epoch: 4/50 Loss: 0.8661        \n",
      "Phase: train Epoch: 5/50 Loss: 0.0422        \n",
      "Phase: validation   Epoch: 5/50 Loss: 0.8600        \n",
      "Phase: train Epoch: 6/50 Loss: 0.0379        \n",
      "Phase: validation   Epoch: 6/50 Loss: 0.8602        \n",
      "Phase: train Epoch: 7/50 Loss: 0.0434        \n",
      "Phase: validation   Epoch: 7/50 Loss: 0.8520        \n",
      "Phase: train Epoch: 8/50 Loss: 0.0427        \n",
      "Phase: validation   Epoch: 8/50 Loss: 0.8553        \n",
      "Phase: train Epoch: 9/50 Loss: 0.0421        \n",
      "Phase: validation   Epoch: 9/50 Loss: 0.8387        \n",
      "Phase: train Epoch: 10/50 Loss: 0.0423        \n",
      "Phase: validation   Epoch: 10/50 Loss: 0.8410        \n",
      "Phase: train Epoch: 11/50 Loss: 0.0403        \n",
      "Phase: validation   Epoch: 11/50 Loss: 0.8601        \n",
      "Phase: train Epoch: 12/50 Loss: 0.0426        \n",
      "Phase: validation   Epoch: 12/50 Loss: 0.8326        \n",
      "Phase: train Epoch: 13/50 Loss: 0.0404        \n",
      "Phase: validation   Epoch: 13/50 Loss: 0.8560        \n",
      "Phase: train Epoch: 14/50 Loss: 0.0421        \n",
      "Phase: validation   Epoch: 14/50 Loss: 0.8699        \n",
      "Phase: train Epoch: 15/50 Loss: 0.0360        \n",
      "Phase: validation   Epoch: 15/50 Loss: 0.8823        \n",
      "Phase: train Epoch: 16/50 Loss: 0.0396        \n",
      "Phase: validation   Epoch: 16/50 Loss: 0.8819        \n",
      "Phase: train Epoch: 17/50 Loss: 0.0412        \n",
      "Phase: validation   Epoch: 17/50 Loss: 0.8766        \n",
      "Phase: train Epoch: 18/50 Loss: 0.0456        \n",
      "Phase: validation   Epoch: 18/50 Loss: 0.8660        \n",
      "Phase: train Epoch: 19/50 Loss: 0.0413        \n",
      "Phase: validation   Epoch: 19/50 Loss: 0.8856        \n",
      "Phase: train Epoch: 20/50 Loss: 0.0438        \n",
      "Phase: validation   Epoch: 20/50 Loss: 0.9290        \n",
      "Phase: train Epoch: 21/50 Loss: 0.0486        \n",
      "Phase: validation   Epoch: 21/50 Loss: 0.9071        \n",
      "Phase: train Epoch: 22/50 Loss: 0.0442        \n",
      "Phase: validation   Epoch: 22/50 Loss: 0.8694        \n",
      "Phase: train Epoch: 23/50 Loss: 0.0408        \n",
      "Phase: validation   Epoch: 23/50 Loss: 0.8510        \n",
      "Phase: train Epoch: 24/50 Loss: 0.0390        \n",
      "Phase: validation   Epoch: 24/50 Loss: 0.8750        \n",
      "Phase: train Epoch: 25/50 Loss: 0.0437        \n",
      "Phase: validation   Epoch: 25/50 Loss: 0.8770        \n",
      "Phase: train Epoch: 26/50 Loss: 0.0396        \n",
      "Phase: validation   Epoch: 26/50 Loss: 0.8852        \n",
      "Phase: train Epoch: 27/50 Loss: 0.0450        \n",
      "Phase: validation   Epoch: 27/50 Loss: 0.8672        \n",
      "Phase: train Epoch: 28/50 Loss: 0.0404        \n",
      "Phase: validation   Epoch: 28/50 Loss: 0.8832        \n",
      "Phase: train Epoch: 29/50 Loss: 0.0493        \n",
      "Phase: validation   Epoch: 29/50 Loss: 0.8740        \n",
      "Phase: train Epoch: 30/50 Loss: 0.0466        \n",
      "Phase: validation   Epoch: 30/50 Loss: 0.8836        \n",
      "Phase: train Epoch: 31/50 Loss: 0.0438        \n",
      "Phase: validation   Epoch: 31/50 Loss: 0.8387        \n",
      "Phase: train Epoch: 32/50 Loss: 0.0445        \n",
      "Phase: validation   Epoch: 32/50 Loss: 0.8255        \n",
      "Phase: train Epoch: 33/50 Loss: 0.0402        \n",
      "Phase: validation   Epoch: 33/50 Loss: 0.8327        \n",
      "Phase: train Epoch: 34/50 Loss: 0.0427        \n",
      "Phase: validation   Epoch: 34/50 Loss: 0.8497        \n",
      "Phase: train Epoch: 35/50 Loss: 0.0434        \n",
      "Phase: validation   Epoch: 35/50 Loss: 0.8586        \n",
      "Phase: train Epoch: 36/50 Loss: 0.0446        \n",
      "Phase: validation   Epoch: 36/50 Loss: 0.8994        \n",
      "Phase: train Epoch: 37/50 Loss: 0.0459        \n",
      "Phase: validation   Epoch: 37/50 Loss: 0.8817        \n",
      "Phase: train Epoch: 38/50 Loss: 0.0453        \n",
      "Phase: validation   Epoch: 38/50 Loss: 0.8763        \n",
      "Phase: train Epoch: 39/50 Loss: 0.0392        \n",
      "Phase: validation   Epoch: 39/50 Loss: 0.8702        \n",
      "Phase: train Epoch: 40/50 Loss: 0.0528        \n",
      "Phase: validation   Epoch: 40/50 Loss: 0.8303        \n",
      "Phase: train Epoch: 41/50 Loss: 0.0429        \n",
      "Phase: validation   Epoch: 41/50 Loss: 0.8516        \n",
      "Phase: train Epoch: 42/50 Loss: 0.0404        \n",
      "Phase: validation   Epoch: 42/50 Loss: 0.8869        \n",
      "Phase: train Epoch: 43/50 Loss: 0.0399        \n",
      "Phase: validation   Epoch: 43/50 Loss: 0.8855        \n",
      "Phase: train Epoch: 44/50 Loss: 0.0439        \n",
      "Phase: validation   Epoch: 44/50 Loss: 0.8469        \n",
      "Phase: train Epoch: 45/50 Loss: 0.0381        \n",
      "Phase: validation   Epoch: 45/50 Loss: 0.8610        \n",
      "Phase: train Epoch: 46/50 Loss: 0.0420        \n",
      "Phase: validation   Epoch: 46/50 Loss: 0.8571        \n",
      "Phase: train Epoch: 47/50 Loss: 0.0428        \n",
      "Phase: validation   Epoch: 47/50 Loss: 0.8599        \n",
      "Phase: train Epoch: 48/50 Loss: 0.0423        \n",
      "Phase: validation   Epoch: 48/50 Loss: 0.8775        \n",
      "Phase: train Epoch: 49/50 Loss: 0.0462        \n",
      "Phase: validation   Epoch: 49/50 Loss: 0.8484        \n",
      "Phase: train Epoch: 50/50 Loss: 0.0427        \n",
      "Phase: validation   Epoch: 50/50 Loss: 0.8246        \n",
      "Best test loss: 0.8245\n"
     ]
    }
   ],
   "source": [
    "# Modify training set to be fractional amounts and train/save various models\n",
    "percentages = [10, 20, 40, 60, 80]\n",
    "# percentages = [1, 5]\n",
    "for percentage in percentages:\n",
    "    # Modify training dataloader to only contain a fraction of the original\n",
    "    dataloaders['train'] = downsample_train(x_train, y_train, percentage)\n",
    "    # Train model\n",
    "    model_hybrid = train(model_hybrid, loss_func, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs)\n",
    "    # Save model weights locally in case of GCP crash\n",
    "    torch.save(model_hybrid.state_dict(), f'./weights/{percentage}_hybrid_50epochs.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caeb277-2a23-45b1-9d77-9fc75febc97b",
   "metadata": {},
   "source": [
    "## Test HQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad4d3bad-703e-4c63-85ae-d6de9177f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to test model\n",
    "def test(model, device, test_loader):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Send the data and target to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            preds.append(enc.inverse_transform(output)[0])\n",
    "            targets.append(enc.inverse_transform(target)[0])\n",
    "    return preds, targets\n",
    "\n",
    "# Helper function to load in weights and send to test function\n",
    "def load_and_test(percentage, model_type):\n",
    "    best_weights = torch.load(f'./weights/{percentage}_{model_type}_50epochs.pt')\n",
    "    model_hybrid.load_state_dict(best_weights)\n",
    "    model_hybrid.eval()\n",
    "    preds, targets = test(model_hybrid, 'cpu', dataloaders['test'])\n",
    "    print(classification_report(y_true=targets, y_pred=preds))\n",
    "    return accuracy_score(y_true=targets, y_pred=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9af65918-c458-49e5-acef-d76f70576721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c047cd65f4984be18b58117b72c47799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.98      1.00      0.99        60\n",
      "        COAD       1.00      0.81      0.90        16\n",
      "        KIRC       1.00      1.00      1.00        30\n",
      "        LUAD       0.90      0.93      0.91        28\n",
      "        PRAD       0.96      1.00      0.98        27\n",
      "\n",
      "    accuracy                           0.97       161\n",
      "   macro avg       0.97      0.95      0.96       161\n",
      "weighted avg       0.97      0.97      0.97       161\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68dd6aceafa46a38b490e97f78f37c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.94      1.00      0.97        60\n",
      "        COAD       1.00      0.56      0.72        16\n",
      "        KIRC       0.97      1.00      0.98        30\n",
      "        LUAD       0.90      0.93      0.91        28\n",
      "        PRAD       0.96      1.00      0.98        27\n",
      "\n",
      "    accuracy                           0.94       161\n",
      "   macro avg       0.95      0.90      0.91       161\n",
      "weighted avg       0.95      0.94      0.94       161\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.968944099378882, 0.9440993788819876]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentages = [10, 20, 40, 60, 80]\n",
    "percentages = [1, 5]\n",
    "acc_hybrid = []\n",
    "for percentage in percentages:\n",
    "    acc_hybrid.append(load_and_test(percentage, 'hybrid'))\n",
    "    print()\n",
    "acc_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d6a0cc-f17f-4b78-a70f-654d4f3eec12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
