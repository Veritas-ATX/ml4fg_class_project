{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263a3272-e8cd-4b51-9aaf-932ce45fb06d",
   "metadata": {},
   "source": [
    "# ML4FG Interim Report Classical Code\n",
    "### By: Austin Stiefelmaier 11/11/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f775c36-6b70-41d0-8e66-e17f5b94e34f",
   "metadata": {},
   "source": [
    "## Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbadb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcb8f46-b67f-4c99-afdf-0a6a980f1dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Fix random num generation for reproducibility \n",
    "torch.manual_seed(7)\n",
    "np.random.seed(7)\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# If GPU available, set to run on it\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94266a11-f651-4e30-9fb5-885f6715ffe7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d566b0e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Data download\n",
    "df_x = pd.read_csv('/home/as6734/ml4fg_class_project/TCGA-PANCAN-HiSeq-801x20531/data.csv')\n",
    "df_y = pd.read_csv('/home/as6734/ml4fg_class_project/TCGA-PANCAN-HiSeq-801x20531/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a746ad56-e2cc-44e9-aa3f-a23a466154d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>gene_0</th>\n",
       "      <th>gene_1</th>\n",
       "      <th>gene_2</th>\n",
       "      <th>gene_3</th>\n",
       "      <th>gene_4</th>\n",
       "      <th>gene_5</th>\n",
       "      <th>gene_6</th>\n",
       "      <th>gene_7</th>\n",
       "      <th>gene_8</th>\n",
       "      <th>...</th>\n",
       "      <th>gene_20521</th>\n",
       "      <th>gene_20522</th>\n",
       "      <th>gene_20523</th>\n",
       "      <th>gene_20524</th>\n",
       "      <th>gene_20525</th>\n",
       "      <th>gene_20526</th>\n",
       "      <th>gene_20527</th>\n",
       "      <th>gene_20528</th>\n",
       "      <th>gene_20529</th>\n",
       "      <th>gene_20530</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.017209</td>\n",
       "      <td>3.265527</td>\n",
       "      <td>5.478487</td>\n",
       "      <td>10.431999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.175175</td>\n",
       "      <td>0.591871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.926711</td>\n",
       "      <td>8.210257</td>\n",
       "      <td>9.723516</td>\n",
       "      <td>7.220030</td>\n",
       "      <td>9.119813</td>\n",
       "      <td>12.003135</td>\n",
       "      <td>9.650743</td>\n",
       "      <td>8.921326</td>\n",
       "      <td>5.286759</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.592732</td>\n",
       "      <td>1.588421</td>\n",
       "      <td>7.586157</td>\n",
       "      <td>9.623011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.816049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.593372</td>\n",
       "      <td>7.323865</td>\n",
       "      <td>9.740931</td>\n",
       "      <td>6.256586</td>\n",
       "      <td>8.381612</td>\n",
       "      <td>12.674552</td>\n",
       "      <td>10.517059</td>\n",
       "      <td>9.397854</td>\n",
       "      <td>2.094168</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.511759</td>\n",
       "      <td>4.327199</td>\n",
       "      <td>6.881787</td>\n",
       "      <td>9.870730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.972130</td>\n",
       "      <td>0.452595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.125213</td>\n",
       "      <td>8.127123</td>\n",
       "      <td>10.908640</td>\n",
       "      <td>5.401607</td>\n",
       "      <td>9.911597</td>\n",
       "      <td>9.045255</td>\n",
       "      <td>9.788359</td>\n",
       "      <td>10.090470</td>\n",
       "      <td>1.683023</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.663618</td>\n",
       "      <td>4.507649</td>\n",
       "      <td>6.659068</td>\n",
       "      <td>10.196184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.843375</td>\n",
       "      <td>0.434882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.076566</td>\n",
       "      <td>8.792959</td>\n",
       "      <td>10.141520</td>\n",
       "      <td>8.942805</td>\n",
       "      <td>9.601208</td>\n",
       "      <td>11.392682</td>\n",
       "      <td>9.694814</td>\n",
       "      <td>9.684365</td>\n",
       "      <td>3.292001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.655741</td>\n",
       "      <td>2.821547</td>\n",
       "      <td>6.539454</td>\n",
       "      <td>9.738265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.566967</td>\n",
       "      <td>0.360982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.996032</td>\n",
       "      <td>8.891425</td>\n",
       "      <td>10.373790</td>\n",
       "      <td>7.181162</td>\n",
       "      <td>9.846910</td>\n",
       "      <td>11.922439</td>\n",
       "      <td>9.217749</td>\n",
       "      <td>9.461191</td>\n",
       "      <td>5.110372</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 20532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  gene_0    gene_1    gene_2    gene_3     gene_4  gene_5  \\\n",
       "0   sample_0     0.0  2.017209  3.265527  5.478487  10.431999     0.0   \n",
       "1   sample_1     0.0  0.592732  1.588421  7.586157   9.623011     0.0   \n",
       "2   sample_2     0.0  3.511759  4.327199  6.881787   9.870730     0.0   \n",
       "3   sample_3     0.0  3.663618  4.507649  6.659068  10.196184     0.0   \n",
       "4   sample_4     0.0  2.655741  2.821547  6.539454   9.738265     0.0   \n",
       "\n",
       "     gene_6    gene_7  gene_8  ...  gene_20521  gene_20522  gene_20523  \\\n",
       "0  7.175175  0.591871     0.0  ...    4.926711    8.210257    9.723516   \n",
       "1  6.816049  0.000000     0.0  ...    4.593372    7.323865    9.740931   \n",
       "2  6.972130  0.452595     0.0  ...    5.125213    8.127123   10.908640   \n",
       "3  7.843375  0.434882     0.0  ...    6.076566    8.792959   10.141520   \n",
       "4  6.566967  0.360982     0.0  ...    5.996032    8.891425   10.373790   \n",
       "\n",
       "   gene_20524  gene_20525  gene_20526  gene_20527  gene_20528  gene_20529  \\\n",
       "0    7.220030    9.119813   12.003135    9.650743    8.921326    5.286759   \n",
       "1    6.256586    8.381612   12.674552   10.517059    9.397854    2.094168   \n",
       "2    5.401607    9.911597    9.045255    9.788359   10.090470    1.683023   \n",
       "3    8.942805    9.601208   11.392682    9.694814    9.684365    3.292001   \n",
       "4    7.181162    9.846910   11.922439    9.217749    9.461191    5.110372   \n",
       "\n",
       "   gene_20530  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "\n",
       "[5 rows x 20532 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25dbea41-439d-4299-b1c5-ad1a2bbf4d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_0</td>\n",
       "      <td>PRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_1</td>\n",
       "      <td>LUAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_2</td>\n",
       "      <td>PRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_3</td>\n",
       "      <td>PRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_4</td>\n",
       "      <td>BRCA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 Class\n",
       "0   sample_0  PRAD\n",
       "1   sample_1  LUAD\n",
       "2   sample_2  PRAD\n",
       "3   sample_3  PRAD\n",
       "4   sample_4  BRCA"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c85d30-ced3-476c-9121-29c57dbe33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "\n",
    "# Drop first column that only notes sample number (which can be reconstructed from index if need be\n",
    "df_x.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_y.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "# Remove columns with all 0.0 values\n",
    "df_x = df_x.loc[:, (df_x != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4dc4c81-aa50-41a8-a112-e0b8e69d2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "\n",
    "# Normalize values by mean\n",
    "df_x=(df_x-df_x.mean())/df_x.std()\n",
    "# Encode classes\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(df_y.values)\n",
    "y = enc.transform(df_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fceb70b6-fa92-4f05-9dfa-c13a0362017a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_0</th>\n",
       "      <th>gene_1</th>\n",
       "      <th>gene_2</th>\n",
       "      <th>gene_3</th>\n",
       "      <th>gene_4</th>\n",
       "      <th>gene_6</th>\n",
       "      <th>gene_7</th>\n",
       "      <th>gene_8</th>\n",
       "      <th>gene_9</th>\n",
       "      <th>gene_10</th>\n",
       "      <th>...</th>\n",
       "      <th>gene_20521</th>\n",
       "      <th>gene_20522</th>\n",
       "      <th>gene_20523</th>\n",
       "      <th>gene_20524</th>\n",
       "      <th>gene_20525</th>\n",
       "      <th>gene_20526</th>\n",
       "      <th>gene_20527</th>\n",
       "      <th>gene_20528</th>\n",
       "      <th>gene_20529</th>\n",
       "      <th>gene_20530</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>-0.827513</td>\n",
       "      <td>0.159701</td>\n",
       "      <td>-1.947061</td>\n",
       "      <td>1.220812</td>\n",
       "      <td>-0.207838</td>\n",
       "      <td>0.180797</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.082063</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.299388</td>\n",
       "      <td>-0.921179</td>\n",
       "      <td>-0.877290</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>-1.165344</td>\n",
       "      <td>0.389198</td>\n",
       "      <td>-0.869023</td>\n",
       "      <td>-1.187196</td>\n",
       "      <td>-0.116410</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>-2.013759</td>\n",
       "      <td>-1.414158</td>\n",
       "      <td>1.352264</td>\n",
       "      <td>-0.376283</td>\n",
       "      <td>-0.531890</td>\n",
       "      <td>-0.982474</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.586397</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.745985</td>\n",
       "      <td>-2.390720</td>\n",
       "      <td>-0.831373</td>\n",
       "      <td>0.591280</td>\n",
       "      <td>-2.548006</td>\n",
       "      <td>1.390759</td>\n",
       "      <td>0.623162</td>\n",
       "      <td>-0.342063</td>\n",
       "      <td>-1.655854</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>0.417087</td>\n",
       "      <td>1.156013</td>\n",
       "      <td>0.249651</td>\n",
       "      <td>0.112761</td>\n",
       "      <td>-0.391053</td>\n",
       "      <td>-0.092937</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.586397</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.033442</td>\n",
       "      <td>-1.059008</td>\n",
       "      <td>2.247399</td>\n",
       "      <td>0.232456</td>\n",
       "      <td>0.317682</td>\n",
       "      <td>-4.023107</td>\n",
       "      <td>-0.631986</td>\n",
       "      <td>0.886307</td>\n",
       "      <td>-1.854106</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>0.543549</td>\n",
       "      <td>1.325354</td>\n",
       "      <td>-0.098991</td>\n",
       "      <td>0.755269</td>\n",
       "      <td>0.395101</td>\n",
       "      <td>-0.127752</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.586397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241148</td>\n",
       "      <td>0.044877</td>\n",
       "      <td>0.224815</td>\n",
       "      <td>1.718651</td>\n",
       "      <td>-0.263682</td>\n",
       "      <td>-0.521421</td>\n",
       "      <td>-0.793113</td>\n",
       "      <td>0.166070</td>\n",
       "      <td>-1.078268</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>-0.295770</td>\n",
       "      <td>-0.256947</td>\n",
       "      <td>-0.286234</td>\n",
       "      <td>-0.148750</td>\n",
       "      <td>-0.756645</td>\n",
       "      <td>-0.272995</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.586397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133251</td>\n",
       "      <td>0.208122</td>\n",
       "      <td>0.837216</td>\n",
       "      <td>0.979312</td>\n",
       "      <td>0.196522</td>\n",
       "      <td>0.268824</td>\n",
       "      <td>-1.614832</td>\n",
       "      <td>-0.229734</td>\n",
       "      <td>-0.201463</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 20264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gene_0    gene_1    gene_2    gene_3    gene_4    gene_6    gene_7  \\\n",
       "0 -0.194678 -0.827513  0.159701 -1.947061  1.220812 -0.207838  0.180797   \n",
       "1 -0.194678 -2.013759 -1.414158  1.352264 -0.376283 -0.531890 -0.982474   \n",
       "2 -0.194678  0.417087  1.156013  0.249651  0.112761 -0.391053 -0.092937   \n",
       "3 -0.194678  0.543549  1.325354 -0.098991  0.755269  0.395101 -0.127752   \n",
       "4 -0.194678 -0.295770 -0.256947 -0.286234 -0.148750 -0.756645 -0.272995   \n",
       "\n",
       "     gene_8    gene_9   gene_10  ...  gene_20521  gene_20522  gene_20523  \\\n",
       "0 -0.125297 -0.065592 -0.082063  ...   -1.299388   -0.921179   -0.877290   \n",
       "1 -0.125297 -0.065592 -0.586397  ...   -1.745985   -2.390720   -0.831373   \n",
       "2 -0.125297 -0.065592 -0.586397  ...   -1.033442   -1.059008    2.247399   \n",
       "3 -0.125297 -0.065592 -0.586397  ...    0.241148    0.044877    0.224815   \n",
       "4 -0.125297 -0.065592 -0.586397  ...    0.133251    0.208122    0.837216   \n",
       "\n",
       "   gene_20524  gene_20525  gene_20526  gene_20527  gene_20528  gene_20529  \\\n",
       "0    0.995625   -1.165344    0.389198   -0.869023   -1.187196   -0.116410   \n",
       "1    0.591280   -2.548006    1.390759    0.623162   -0.342063   -1.655854   \n",
       "2    0.232456    0.317682   -4.023107   -0.631986    0.886307   -1.854106   \n",
       "3    1.718651   -0.263682   -0.521421   -0.793113    0.166070   -1.078268   \n",
       "4    0.979312    0.196522    0.268824   -1.614832   -0.229734   -0.201463   \n",
       "\n",
       "   gene_20530  \n",
       "0   -0.261738  \n",
       "1   -0.261738  \n",
       "2   -0.261738  \n",
       "3   -0.261738  \n",
       "4   -0.261738  \n",
       "\n",
       "[5 rows x 20264 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe1cdcb-124d-4741-8854-c379f7b4c654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92ec11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 512, 'validation': 128, 'test': 161}\n"
     ]
    }
   ],
   "source": [
    "# Split data, final percentages are approximately 64% train, 16% validation, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x.values, y, test_size=0.2, train_size=0.8, random_state=7, stratify=y)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, train_size=0.8, random_state=7, stratify=y_train)\n",
    "dataset_sizes = {'train': len(x_train), 'validation': len(x_valid), 'test': len(x_test)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22283318-279b-4514-8a88-dfc8ef9f47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoaders\n",
    "x_train_to_tensor = torch.from_numpy(x_train).to(torch.float32).to(device)\n",
    "y_train_to_tensor = torch.from_numpy(y_train).to(torch.float32).to(device)\n",
    "x_valid_to_tensor = torch.from_numpy(x_valid).to(torch.float32).to(device)\n",
    "y_valid_to_tensor = torch.from_numpy(y_valid).to(torch.float32).to(device)\n",
    "x_test_to_tensor = torch.from_numpy(x_test).to(torch.float32).to(device)\n",
    "y_test_to_tensor = torch.from_numpy(y_test).to(torch.float32).to(device)\n",
    "\n",
    "# Second step: Creating TensorDataset for Dataloader\n",
    "train_set = TensorDataset(x_train_to_tensor, y_train_to_tensor)\n",
    "valid_set = TensorDataset(x_valid_to_tensor, y_valid_to_tensor)\n",
    "test_set = TensorDataset(x_test_to_tensor, y_test_to_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True),\n",
    "    'validation': torch.utils.data.DataLoader(valid_set, batch_size=8),\n",
    "    'test': torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=8)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916720a3-5e54-4bee-b428-7be9e3252bdc",
   "metadata": {},
   "source": [
    "## Classical Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c56560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Model Class\n",
    "class ClassicalNet(nn.Module):\n",
    "    # Initialize layers\n",
    "    def __init__(self, num_feature):\n",
    "        super().__init__()\n",
    "        # Layers before quantum circuit\n",
    "        self.layer_1 = nn.Linear(num_feature, 512)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.layer_2 = nn.Linear(512, 128)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.layer_3 = nn.Linear(128, 64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.layer_4 = nn.Linear(64, 5)\n",
    "        self.c1 = nn.Conv1d(8, 8, 2)\n",
    "        self.layer_5 = nn.Linear(4, 5)\n",
    "        # Misc\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    # Forward pass procedure\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third layer\n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Quantum stand in layers \n",
    "        x = self.layer_4(x)\n",
    "        x = self.c1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.layer_5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "036f4bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassicalNet(\n",
      "  (layer_1): Linear(in_features=20264, out_features=512, bias=True)\n",
      "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_4): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (c1): Conv1d(8, 8, kernel_size=(2,), stride=(1,))\n",
      "  (layer_5): Linear(in_features=4, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = ClassicalNet(num_feature=len(df_x.columns))\n",
    "# Make sure set to use GPU\n",
    "model = model.to(device)\n",
    "# Show model architecture summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ef09c-cd06-41b8-bd44-35bded59fab8",
   "metadata": {},
   "source": [
    "## Train HQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1b52f9-0e31-4ea4-8edb-41272c82a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.0004  # Initial learning rate\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "gamma_lr_scheduler = 0.1  # Learning rate decay param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24238e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss, optimizer, and learning rate decay manager\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=step)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "    optimizer, step_size=10, gamma=gamma_lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "035d2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model(s)\n",
    "def train(model, loss_func, optimizer, scheduler, num_epochs):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100000.0\n",
    "    best_loss_train = 100000.0\n",
    "    print('Training started:')\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                # Set model to train mode\n",
    "                model.train()\n",
    "            else:\n",
    "                # Set model to eval mode\n",
    "                model.eval()\n",
    "            current_loss = 0.0\n",
    "            n_batches = dataset_sizes[phase] // batch_size\n",
    "            iter = 0\n",
    "            for X, Y in dataloaders[phase]:\n",
    "                batch_len = len(X)\n",
    "                X = X.to(device)\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                # If in train mode, get loss, step optimizer\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(X)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = loss_func(outputs, Y)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # Print iteration results\n",
    "                current_loss += loss.item() * batch_len\n",
    "                print('Phase: {} Epoch: {}/{} Iter: {}/{}'.format(phase, epoch+1, num_epochs, iter+1, n_batches+1),\n",
    "                    end=\"\\r\",\n",
    "                    flush=True)\n",
    "                iter += 1\n",
    "\n",
    "            # Get epoch stats and print\n",
    "            epoch_loss = current_loss / dataset_sizes[phase]\n",
    "            print('Phase: {} Epoch: {}/{} Loss: {:.4f}        '.format(\n",
    "                    'train' if phase == 'train' else 'validation  ',\n",
    "                    epoch + 1,\n",
    "                    num_epochs,\n",
    "                    epoch_loss,\n",
    "                )\n",
    "            )\n",
    "            # Update best vars and make copy of best weights\n",
    "            # if phase == \"validation\":\n",
    "            #     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"validation\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
    "                best_loss_train = epoch_loss\n",
    "            # Decay learning rate\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "    # Print final results\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Best test loss: {:.4f}'.format(best_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ade40fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9782050b1f834556b52b12e246fbd89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train Epoch: 1/50 Loss: 1.6816        \n",
      "Phase: validation   Epoch: 1/50 Loss: 1.6877        \n",
      "Phase: train Epoch: 2/50 Loss: 1.6637        \n",
      "Phase: validation   Epoch: 2/50 Loss: 1.6592        \n",
      "Phase: train Epoch: 3/50 Loss: 1.6219        \n",
      "Phase: validation   Epoch: 3/50 Loss: 1.6300        \n",
      "Phase: train Epoch: 4/50 Loss: 1.6135        \n",
      "Phase: validation   Epoch: 4/50 Loss: 1.6055        \n",
      "Phase: train Epoch: 5/50 Loss: 1.5960        \n",
      "Phase: validation   Epoch: 5/50 Loss: 1.5686        \n",
      "Phase: train Epoch: 6/50 Loss: 1.5250        \n",
      "Phase: validation   Epoch: 6/50 Loss: 1.5307        \n",
      "Phase: train Epoch: 7/50 Loss: 1.4408        \n",
      "Phase: validation   Epoch: 7/50 Loss: 1.4369        \n",
      "Phase: train Epoch: 8/50 Loss: 1.3299        \n",
      "Phase: validation   Epoch: 8/50 Loss: 1.2895        \n",
      "Phase: train Epoch: 9/50 Loss: 1.2189        \n",
      "Phase: validation   Epoch: 9/50 Loss: 1.1618        \n",
      "Phase: train Epoch: 10/50 Loss: 1.0846        \n",
      "Phase: validation   Epoch: 10/50 Loss: 0.9957        \n",
      "Phase: train Epoch: 11/50 Loss: 0.9903        \n",
      "Phase: validation   Epoch: 11/50 Loss: 0.9745        \n",
      "Phase: train Epoch: 12/50 Loss: 0.9146        \n",
      "Phase: validation   Epoch: 12/50 Loss: 0.9593        \n",
      "Phase: train Epoch: 13/50 Loss: 0.9272        \n",
      "Phase: validation   Epoch: 13/50 Loss: 0.9495        \n",
      "Phase: train Epoch: 14/50 Loss: 0.9287        \n",
      "Phase: validation   Epoch: 14/50 Loss: 0.9176        \n",
      "Phase: train Epoch: 15/50 Loss: 0.9277        \n",
      "Phase: validation   Epoch: 15/50 Loss: 0.9025        \n",
      "Phase: train Epoch: 16/50 Loss: 0.8672        \n",
      "Phase: validation   Epoch: 16/50 Loss: 0.8911        \n",
      "Phase: train Epoch: 17/50 Loss: 0.8734        \n",
      "Phase: validation   Epoch: 17/50 Loss: 0.8700        \n",
      "Phase: train Epoch: 18/50 Loss: 0.8237        \n",
      "Phase: validation   Epoch: 18/50 Loss: 0.8631        \n",
      "Phase: train Epoch: 19/50 Loss: 0.8373        \n",
      "Phase: validation   Epoch: 19/50 Loss: 0.8190        \n",
      "Phase: train Epoch: 20/50 Loss: 0.8323        \n",
      "Phase: validation   Epoch: 20/50 Loss: 0.8276        \n",
      "Phase: train Epoch: 21/50 Loss: 0.8538        \n",
      "Phase: validation   Epoch: 21/50 Loss: 0.8087        \n",
      "Phase: train Epoch: 22/50 Loss: 0.8015        \n",
      "Phase: validation   Epoch: 22/50 Loss: 0.8166        \n",
      "Phase: train Epoch: 23/50 Loss: 0.8494        \n",
      "Phase: validation   Epoch: 23/50 Loss: 0.8101        \n",
      "Phase: train Epoch: 24/50 Loss: 0.8128        \n",
      "Phase: validation   Epoch: 24/50 Loss: 0.8220        \n",
      "Phase: train Epoch: 25/50 Loss: 0.8415        \n",
      "Phase: validation   Epoch: 25/50 Loss: 0.8269        \n",
      "Phase: train Epoch: 26/50 Loss: 0.8347        \n",
      "Phase: validation   Epoch: 26/50 Loss: 0.8129        \n",
      "Phase: train Epoch: 27/50 Loss: 0.8274        \n",
      "Phase: validation   Epoch: 27/50 Loss: 0.8207        \n",
      "Phase: train Epoch: 28/50 Loss: 0.8510        \n",
      "Phase: validation   Epoch: 28/50 Loss: 0.8057        \n",
      "Phase: train Epoch: 29/50 Loss: 0.7859        \n",
      "Phase: validation   Epoch: 29/50 Loss: 0.8203        \n",
      "Phase: train Epoch: 30/50 Loss: 0.8118        \n",
      "Phase: validation   Epoch: 30/50 Loss: 0.8253        \n",
      "Phase: train Epoch: 31/50 Loss: 0.7909        \n",
      "Phase: validation   Epoch: 31/50 Loss: 0.8063        \n",
      "Phase: train Epoch: 32/50 Loss: 0.8671        \n",
      "Phase: validation   Epoch: 32/50 Loss: 0.7761        \n",
      "Phase: train Epoch: 33/50 Loss: 0.7998        \n",
      "Phase: validation   Epoch: 33/50 Loss: 0.8158        \n",
      "Phase: train Epoch: 34/50 Loss: 0.8083        \n",
      "Phase: validation   Epoch: 34/50 Loss: 0.8178        \n",
      "Phase: train Epoch: 35/50 Loss: 0.8367        \n",
      "Phase: validation   Epoch: 35/50 Loss: 0.8203        \n",
      "Phase: train Epoch: 36/50 Loss: 0.8313        \n",
      "Phase: validation   Epoch: 36/50 Loss: 0.7947        \n",
      "Phase: train Epoch: 37/50 Loss: 0.7997        \n",
      "Phase: validation   Epoch: 37/50 Loss: 0.8247        \n",
      "Phase: train Epoch: 38/50 Loss: 0.8295        \n",
      "Phase: validation   Epoch: 38/50 Loss: 0.8156        \n",
      "Phase: train Epoch: 39/50 Loss: 0.8498        \n",
      "Phase: validation   Epoch: 39/50 Loss: 0.8082        \n",
      "Phase: train Epoch: 40/50 Loss: 0.8196        \n",
      "Phase: validation   Epoch: 40/50 Loss: 0.8063        \n",
      "Phase: train Epoch: 41/50 Loss: 0.7738        \n",
      "Phase: validation   Epoch: 41/50 Loss: 0.8238        \n",
      "Phase: train Epoch: 42/50 Loss: 0.8164        \n",
      "Phase: validation   Epoch: 42/50 Loss: 0.7747        \n",
      "Phase: train Epoch: 43/50 Loss: 0.8443        \n",
      "Phase: validation   Epoch: 43/50 Loss: 0.8092        \n",
      "Phase: train Epoch: 44/50 Loss: 0.7803        \n",
      "Phase: validation   Epoch: 44/50 Loss: 0.8187        \n",
      "Phase: train Epoch: 45/50 Loss: 0.8115        \n",
      "Phase: validation   Epoch: 45/50 Loss: 0.8134        \n",
      "Phase: train Epoch: 46/50 Loss: 0.8112        \n",
      "Phase: validation   Epoch: 46/50 Loss: 0.8113        \n",
      "Phase: train Epoch: 47/50 Loss: 0.8391        \n",
      "Phase: validation   Epoch: 47/50 Loss: 0.7990        \n",
      "Phase: train Epoch: 48/50 Loss: 0.8130        \n",
      "Phase: validation   Epoch: 48/50 Loss: 0.8094        \n",
      "Phase: train Epoch: 49/50 Loss: 0.8486        \n",
      "Phase: validation   Epoch: 49/50 Loss: 0.8053        \n",
      "Phase: train Epoch: 50/50 Loss: 0.8001        \n",
      "Phase: validation   Epoch: 50/50 Loss: 0.8268        \n",
      "Best test loss: 0.7747\n"
     ]
    }
   ],
   "source": [
    "model = train(model, loss_func, optimizer, exp_lr_scheduler, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e7672ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights locally in case of GCP crash\n",
    "torch.save(model.state_dict(), './weights/classical_50epochs.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd073d9f-88a8-43c1-a147-1a6ca3baef3c",
   "metadata": {},
   "source": [
    "## Test Classical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef720e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in saved weights, put in model, and than set to eval mode\n",
    "best_weights = torch.load('./weights/classical_50epochs.pt')\n",
    "model.load_state_dict(best_weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9e278d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a1d7f3894949019ae1150ab3d7e1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function to test model\n",
    "def test(model, device, test_loader):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Send the data and target to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if data.size()[0] < 8:\n",
    "                break\n",
    "            output = model(data)\n",
    "            preds += enc.inverse_transform(output).flatten().tolist()\n",
    "            targets += enc.inverse_transform(target).flatten().tolist()\n",
    "    return preds, targets\n",
    "\n",
    "preds, targets = test(model, 'cpu', dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25df5fd9-98b4-4b3d-a686-ffde8a25ca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.79      0.87      0.83        60\n",
      "        COAD       1.00      0.19      0.32        16\n",
      "        KIRC       0.78      0.93      0.85        30\n",
      "        LUAD       0.70      0.57      0.63        28\n",
      "        PRAD       0.69      0.85      0.76        26\n",
      "\n",
      "    accuracy                           0.76       160\n",
      "   macro avg       0.79      0.68      0.68       160\n",
      "weighted avg       0.77      0.76      0.73       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=targets, y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2bb04-b75c-4927-9797-e68cbcfa0883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
