{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263a3272-e8cd-4b51-9aaf-932ce45fb06d",
   "metadata": {},
   "source": [
    "# ML4FG Interim Report HQNN Code\n",
    "### By: Austin Stiefelmaier 11/11/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f775c36-6b70-41d0-8e66-e17f5b94e34f",
   "metadata": {},
   "source": [
    "## Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by quantum ML code from:\n",
    "# https://pennylane.ai/qml/demos/tutorial_quantum_transfer_learning.html\n",
    "# And classical ML code from:\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbadb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Pennylane imports\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcb8f46-b67f-4c99-afdf-0a6a980f1dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Fix random num generation for reproducibility \n",
    "torch.manual_seed(7)\n",
    "np.random.seed(7)\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# If GPU available, set to run on it\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94266a11-f651-4e30-9fb5-885f6715ffe7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d566b0e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Data download\n",
    "df_x = pd.read_csv('/home/as6734/ml4fg_class_project/TCGA-PANCAN-HiSeq-801x20531/data.csv')\n",
    "df_y = pd.read_csv('/home/as6734/ml4fg_class_project/TCGA-PANCAN-HiSeq-801x20531/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a746ad56-e2cc-44e9-aa3f-a23a466154d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>gene_0</th>\n",
       "      <th>gene_1</th>\n",
       "      <th>gene_2</th>\n",
       "      <th>gene_3</th>\n",
       "      <th>gene_4</th>\n",
       "      <th>gene_5</th>\n",
       "      <th>gene_6</th>\n",
       "      <th>gene_7</th>\n",
       "      <th>gene_8</th>\n",
       "      <th>...</th>\n",
       "      <th>gene_20521</th>\n",
       "      <th>gene_20522</th>\n",
       "      <th>gene_20523</th>\n",
       "      <th>gene_20524</th>\n",
       "      <th>gene_20525</th>\n",
       "      <th>gene_20526</th>\n",
       "      <th>gene_20527</th>\n",
       "      <th>gene_20528</th>\n",
       "      <th>gene_20529</th>\n",
       "      <th>gene_20530</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.017209</td>\n",
       "      <td>3.265527</td>\n",
       "      <td>5.478487</td>\n",
       "      <td>10.431999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.175175</td>\n",
       "      <td>0.591871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.926711</td>\n",
       "      <td>8.210257</td>\n",
       "      <td>9.723516</td>\n",
       "      <td>7.220030</td>\n",
       "      <td>9.119813</td>\n",
       "      <td>12.003135</td>\n",
       "      <td>9.650743</td>\n",
       "      <td>8.921326</td>\n",
       "      <td>5.286759</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.592732</td>\n",
       "      <td>1.588421</td>\n",
       "      <td>7.586157</td>\n",
       "      <td>9.623011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.816049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.593372</td>\n",
       "      <td>7.323865</td>\n",
       "      <td>9.740931</td>\n",
       "      <td>6.256586</td>\n",
       "      <td>8.381612</td>\n",
       "      <td>12.674552</td>\n",
       "      <td>10.517059</td>\n",
       "      <td>9.397854</td>\n",
       "      <td>2.094168</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.511759</td>\n",
       "      <td>4.327199</td>\n",
       "      <td>6.881787</td>\n",
       "      <td>9.870730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.972130</td>\n",
       "      <td>0.452595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.125213</td>\n",
       "      <td>8.127123</td>\n",
       "      <td>10.908640</td>\n",
       "      <td>5.401607</td>\n",
       "      <td>9.911597</td>\n",
       "      <td>9.045255</td>\n",
       "      <td>9.788359</td>\n",
       "      <td>10.090470</td>\n",
       "      <td>1.683023</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.663618</td>\n",
       "      <td>4.507649</td>\n",
       "      <td>6.659068</td>\n",
       "      <td>10.196184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.843375</td>\n",
       "      <td>0.434882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.076566</td>\n",
       "      <td>8.792959</td>\n",
       "      <td>10.141520</td>\n",
       "      <td>8.942805</td>\n",
       "      <td>9.601208</td>\n",
       "      <td>11.392682</td>\n",
       "      <td>9.694814</td>\n",
       "      <td>9.684365</td>\n",
       "      <td>3.292001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.655741</td>\n",
       "      <td>2.821547</td>\n",
       "      <td>6.539454</td>\n",
       "      <td>9.738265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.566967</td>\n",
       "      <td>0.360982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.996032</td>\n",
       "      <td>8.891425</td>\n",
       "      <td>10.373790</td>\n",
       "      <td>7.181162</td>\n",
       "      <td>9.846910</td>\n",
       "      <td>11.922439</td>\n",
       "      <td>9.217749</td>\n",
       "      <td>9.461191</td>\n",
       "      <td>5.110372</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  gene_0    gene_1    gene_2    gene_3     gene_4  gene_5  \\\n",
       "0   sample_0     0.0  2.017209  3.265527  5.478487  10.431999     0.0   \n",
       "1   sample_1     0.0  0.592732  1.588421  7.586157   9.623011     0.0   \n",
       "2   sample_2     0.0  3.511759  4.327199  6.881787   9.870730     0.0   \n",
       "3   sample_3     0.0  3.663618  4.507649  6.659068  10.196184     0.0   \n",
       "4   sample_4     0.0  2.655741  2.821547  6.539454   9.738265     0.0   \n",
       "\n",
       "     gene_6    gene_7  gene_8  ...  gene_20521  gene_20522  gene_20523  \\\n",
       "0  7.175175  0.591871     0.0  ...    4.926711    8.210257    9.723516   \n",
       "1  6.816049  0.000000     0.0  ...    4.593372    7.323865    9.740931   \n",
       "2  6.972130  0.452595     0.0  ...    5.125213    8.127123   10.908640   \n",
       "3  7.843375  0.434882     0.0  ...    6.076566    8.792959   10.141520   \n",
       "4  6.566967  0.360982     0.0  ...    5.996032    8.891425   10.373790   \n",
       "\n",
       "   gene_20524  gene_20525  gene_20526  gene_20527  gene_20528  gene_20529  \\\n",
       "0    7.220030    9.119813   12.003135    9.650743    8.921326    5.286759   \n",
       "1    6.256586    8.381612   12.674552   10.517059    9.397854    2.094168   \n",
       "2    5.401607    9.911597    9.045255    9.788359   10.090470    1.683023   \n",
       "3    8.942805    9.601208   11.392682    9.694814    9.684365    3.292001   \n",
       "4    7.181162    9.846910   11.922439    9.217749    9.461191    5.110372   \n",
       "\n",
       "   gene_20530  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "\n",
       "[5 rows x 20532 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25dbea41-439d-4299-b1c5-ad1a2bbf4d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_0</td>\n",
       "      <td>PRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_1</td>\n",
       "      <td>LUAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_2</td>\n",
       "      <td>PRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_3</td>\n",
       "      <td>PRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_4</td>\n",
       "      <td>BRCA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 Class\n",
       "0   sample_0  PRAD\n",
       "1   sample_1  LUAD\n",
       "2   sample_2  PRAD\n",
       "3   sample_3  PRAD\n",
       "4   sample_4  BRCA"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c85d30-ced3-476c-9121-29c57dbe33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "\n",
    "# Drop first column that only notes sample number (which can be reconstructed from index if need be\n",
    "df_x.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_y.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "# Remove columns with all 0.0 values\n",
    "df_x = df_x.loc[:, (df_x != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4dc4c81-aa50-41a8-a112-e0b8e69d2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "\n",
    "# Normalize values by mean\n",
    "df_x=(df_x-df_x.mean())/df_x.std()\n",
    "# Encode classes\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(df_y.values)\n",
    "y = enc.transform(df_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fceb70b6-fa92-4f05-9dfa-c13a0362017a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_0</th>\n",
       "      <th>gene_1</th>\n",
       "      <th>gene_2</th>\n",
       "      <th>gene_3</th>\n",
       "      <th>gene_4</th>\n",
       "      <th>gene_6</th>\n",
       "      <th>gene_7</th>\n",
       "      <th>gene_8</th>\n",
       "      <th>gene_9</th>\n",
       "      <th>gene_10</th>\n",
       "      <th>...</th>\n",
       "      <th>gene_20521</th>\n",
       "      <th>gene_20522</th>\n",
       "      <th>gene_20523</th>\n",
       "      <th>gene_20524</th>\n",
       "      <th>gene_20525</th>\n",
       "      <th>gene_20526</th>\n",
       "      <th>gene_20527</th>\n",
       "      <th>gene_20528</th>\n",
       "      <th>gene_20529</th>\n",
       "      <th>gene_20530</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>-0.827513</td>\n",
       "      <td>0.159701</td>\n",
       "      <td>-1.947061</td>\n",
       "      <td>1.220812</td>\n",
       "      <td>-0.207838</td>\n",
       "      <td>0.180797</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.082063</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.299388</td>\n",
       "      <td>-0.921179</td>\n",
       "      <td>-0.877290</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>-1.165344</td>\n",
       "      <td>0.389198</td>\n",
       "      <td>-0.869023</td>\n",
       "      <td>-1.187196</td>\n",
       "      <td>-0.116410</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>-2.013759</td>\n",
       "      <td>-1.414158</td>\n",
       "      <td>1.352264</td>\n",
       "      <td>-0.376283</td>\n",
       "      <td>-0.531890</td>\n",
       "      <td>-0.982474</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.586397</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.745985</td>\n",
       "      <td>-2.390720</td>\n",
       "      <td>-0.831373</td>\n",
       "      <td>0.591280</td>\n",
       "      <td>-2.548006</td>\n",
       "      <td>1.390759</td>\n",
       "      <td>0.623162</td>\n",
       "      <td>-0.342063</td>\n",
       "      <td>-1.655854</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>0.417087</td>\n",
       "      <td>1.156013</td>\n",
       "      <td>0.249651</td>\n",
       "      <td>0.112761</td>\n",
       "      <td>-0.391053</td>\n",
       "      <td>-0.092937</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.586397</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.033442</td>\n",
       "      <td>-1.059008</td>\n",
       "      <td>2.247399</td>\n",
       "      <td>0.232456</td>\n",
       "      <td>0.317682</td>\n",
       "      <td>-4.023107</td>\n",
       "      <td>-0.631986</td>\n",
       "      <td>0.886307</td>\n",
       "      <td>-1.854106</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>0.543549</td>\n",
       "      <td>1.325354</td>\n",
       "      <td>-0.098991</td>\n",
       "      <td>0.755269</td>\n",
       "      <td>0.395101</td>\n",
       "      <td>-0.127752</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.586397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241148</td>\n",
       "      <td>0.044877</td>\n",
       "      <td>0.224815</td>\n",
       "      <td>1.718651</td>\n",
       "      <td>-0.263682</td>\n",
       "      <td>-0.521421</td>\n",
       "      <td>-0.793113</td>\n",
       "      <td>0.166070</td>\n",
       "      <td>-1.078268</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.194678</td>\n",
       "      <td>-0.295770</td>\n",
       "      <td>-0.256947</td>\n",
       "      <td>-0.286234</td>\n",
       "      <td>-0.148750</td>\n",
       "      <td>-0.756645</td>\n",
       "      <td>-0.272995</td>\n",
       "      <td>-0.125297</td>\n",
       "      <td>-0.065592</td>\n",
       "      <td>-0.586397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133251</td>\n",
       "      <td>0.208122</td>\n",
       "      <td>0.837216</td>\n",
       "      <td>0.979312</td>\n",
       "      <td>0.196522</td>\n",
       "      <td>0.268824</td>\n",
       "      <td>-1.614832</td>\n",
       "      <td>-0.229734</td>\n",
       "      <td>-0.201463</td>\n",
       "      <td>-0.261738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gene_0    gene_1    gene_2    gene_3    gene_4    gene_6    gene_7  \\\n",
       "0 -0.194678 -0.827513  0.159701 -1.947061  1.220812 -0.207838  0.180797   \n",
       "1 -0.194678 -2.013759 -1.414158  1.352264 -0.376283 -0.531890 -0.982474   \n",
       "2 -0.194678  0.417087  1.156013  0.249651  0.112761 -0.391053 -0.092937   \n",
       "3 -0.194678  0.543549  1.325354 -0.098991  0.755269  0.395101 -0.127752   \n",
       "4 -0.194678 -0.295770 -0.256947 -0.286234 -0.148750 -0.756645 -0.272995   \n",
       "\n",
       "     gene_8    gene_9   gene_10  ...  gene_20521  gene_20522  gene_20523  \\\n",
       "0 -0.125297 -0.065592 -0.082063  ...   -1.299388   -0.921179   -0.877290   \n",
       "1 -0.125297 -0.065592 -0.586397  ...   -1.745985   -2.390720   -0.831373   \n",
       "2 -0.125297 -0.065592 -0.586397  ...   -1.033442   -1.059008    2.247399   \n",
       "3 -0.125297 -0.065592 -0.586397  ...    0.241148    0.044877    0.224815   \n",
       "4 -0.125297 -0.065592 -0.586397  ...    0.133251    0.208122    0.837216   \n",
       "\n",
       "   gene_20524  gene_20525  gene_20526  gene_20527  gene_20528  gene_20529  \\\n",
       "0    0.995625   -1.165344    0.389198   -0.869023   -1.187196   -0.116410   \n",
       "1    0.591280   -2.548006    1.390759    0.623162   -0.342063   -1.655854   \n",
       "2    0.232456    0.317682   -4.023107   -0.631986    0.886307   -1.854106   \n",
       "3    1.718651   -0.263682   -0.521421   -0.793113    0.166070   -1.078268   \n",
       "4    0.979312    0.196522    0.268824   -1.614832   -0.229734   -0.201463   \n",
       "\n",
       "   gene_20530  \n",
       "0   -0.261738  \n",
       "1   -0.261738  \n",
       "2   -0.261738  \n",
       "3   -0.261738  \n",
       "4   -0.261738  \n",
       "\n",
       "[5 rows x 20264 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe1cdcb-124d-4741-8854-c379f7b4c654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92ec11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 512, 'validation': 128, 'test': 161}\n"
     ]
    }
   ],
   "source": [
    "# Split data, final percentages are approximately 64% train, 16% validation, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x.values, y, test_size=0.2, train_size=0.8, random_state=7, stratify=y)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, train_size=0.8, random_state=7, stratify=y_train)\n",
    "dataset_sizes = {'train': len(x_train), 'validation': len(x_valid), 'test': len(x_test)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75216164-2fd3-4af3-832c-0a8fe778ca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Categories (total=512):\n",
      "BRCA: 192, COAD: 50, KIRC: 93, LUAD: 90, PRAD: 87\n",
      "BRCA: 0.375, COAD: 0.098, KIRC: 0.182, LUAD: 0.176, PRAD: 0.170\n",
      "\n",
      "Validation Data Categories (total=128):\n",
      "BRCA: 48, COAD: 12, KIRC: 23, LUAD: 23, PRAD: 22\n",
      "BRCA: 0.375, COAD: 0.094, KIRC: 0.180, LUAD: 0.180, PRAD: 0.172\n",
      "\n",
      "Test Data Categories (total=161):\n",
      "BRCA: 60, COAD: 16, KIRC: 30, LUAD: 28, PRAD: 27\n",
      "BRCA: 0.373, COAD: 0.099, KIRC: 0.186, LUAD: 0.174, PRAD: 0.168\n",
      "\n",
      "All Data Categories (total=801):\n",
      "BRCA: 300, COAD: 78, KIRC: 146, LUAD: 141, PRAD: 136\n",
      "BRCA: 0.375, COAD: 0.097, KIRC: 0.182, LUAD: 0.176, PRAD: 0.170\n"
     ]
    }
   ],
   "source": [
    "# Data split analysis\n",
    "\n",
    "def sort_and_prepprint_dict(input_dict, ints=True):\n",
    "    outstring = ''\n",
    "    # Sort the dictionary by keys in alphabetical order\n",
    "    sorted_dict = {k: input_dict[k] for k in sorted(input_dict)}\n",
    "    # Print key-value pairs with 3 decimal place precision for float values\n",
    "    for key, value in sorted_dict.items():\n",
    "        if ints:\n",
    "            outstring += f\"{key}: {value}, \"\n",
    "        else:\n",
    "            outstring += f\"{key}: {value:.3f}, \"\n",
    "    return outstring[:-2]\n",
    "\n",
    "def get_data_counts(data, datatype, total):\n",
    "    cats = {}\n",
    "    cats_pcts = {}\n",
    "    for i in enc.inverse_transform(data):\n",
    "        if i[0] not in cats:\n",
    "            cats[i[0]] = 1\n",
    "            cats_pcts[i[0]] = 1\n",
    "        else:\n",
    "            cats[i[0]] += 1\n",
    "            cats_pcts[i[0]] += 1\n",
    "    print(f'{datatype} Data Categories (total={total}):')\n",
    "    print(sort_and_prepprint_dict(cats))\n",
    "    for key in cats_pcts:\n",
    "        cats_pcts[key] /= total\n",
    "    print(sort_and_prepprint_dict(cats_pcts, ints=False))\n",
    "\n",
    "get_data_counts(y_train, 'Train', dataset_sizes['train'])\n",
    "print()\n",
    "get_data_counts(y_valid, 'Validation', dataset_sizes['validation'])\n",
    "print()\n",
    "get_data_counts(y_test, 'Test', dataset_sizes['test'])\n",
    "print()\n",
    "get_data_counts(y, 'All', len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22283318-279b-4514-8a88-dfc8ef9f47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoaders\n",
    "x_train_to_tensor = torch.from_numpy(x_train).to(torch.float32).to(device)\n",
    "y_train_to_tensor = torch.from_numpy(y_train).to(torch.float32).to(device)\n",
    "x_valid_to_tensor = torch.from_numpy(x_valid).to(torch.float32).to(device)\n",
    "y_valid_to_tensor = torch.from_numpy(y_valid).to(torch.float32).to(device)\n",
    "x_test_to_tensor = torch.from_numpy(x_test).to(torch.float32).to(device)\n",
    "y_test_to_tensor = torch.from_numpy(y_test).to(torch.float32).to(device)\n",
    "\n",
    "# Second step: Creating TensorDataset for Dataloader\n",
    "train_set = TensorDataset(x_train_to_tensor, y_train_to_tensor)\n",
    "valid_set = TensorDataset(x_valid_to_tensor, y_valid_to_tensor)\n",
    "test_set = TensorDataset(x_test_to_tensor, y_test_to_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True),\n",
    "    'validation': torch.utils.data.DataLoader(valid_set, batch_size=8),\n",
    "    'test': torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40e428",
   "metadata": {},
   "source": [
    "## Quantum Circuit Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4b4fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum circuit params\n",
    "qubit_count = 5\n",
    "circ_repeats = 6  # How many times to repeat RY and CNOT gates\n",
    "q_delta = 0.01  # Param for quantum circuit weight initialization\n",
    "\n",
    "# Run quantum circuit on Pennylane default simulator\n",
    "dev = qml.device('default.qubit', wires=qubit_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ec083a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to construct quantum circuit\n",
    "def ry_gates(w):\n",
    "    # Apply rotation gate RY w/ given weights\n",
    "    for i, weight in enumerate(w):\n",
    "        qml.RY(weight, wires=i)\n",
    "\n",
    "# Adds alternating CNOT layer to entangle qubits\n",
    "def entangling_layer(nqubits):\n",
    "    for i in range(0, nqubits - 1, 2):  # Evens\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Odds\n",
    "        qml.CNOT(wires=[i, i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f695c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct quantum circuit to plug into PyTorch\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def quantum_circuit(inputs, q_weights_flat):\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(circ_repeats, qubit_count)\n",
    "    # Initialize w/ H gates so orthogonal to computational basis\n",
    "    # This helps start w/out bias towards 0 or 1 states\n",
    "    for i in range(qubit_count):\n",
    "        qml.Hadamard(wires=i)\n",
    "    # Take given inputs and apply to quantum circuit as first weights\n",
    "    ry_gates(inputs)\n",
    "    # Repeat CNOT and RY gates to add more weights to train and\n",
    "    # CNOT \"convolutions\" (really entangles)\n",
    "    for k in range(circ_repeats):\n",
    "        entangling_layer(qubit_count)\n",
    "        ry_gates(q_weights[k])\n",
    "    # Use Pennylane sim to get expected value after applying Z gate\n",
    "    # which returns to standard computational basis, this is layer output\n",
    "    expected_vals = [qml.expval(qml.PauliZ(entry)) for entry in range(qubit_count)]\n",
    "    return tuple(expected_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "293270e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──H──RY(-0.15)─╭●──RY(-0.01)────────────╭●──RY(0.00)────────────╭●──RY(-0.01)────────────╭●\n",
      "1: ──H──RY(0.69)──╰X─╭●──────────RY(0.00)──╰X─╭●─────────RY(-0.01)─╰X─╭●──────────RY(-0.00)─╰X\n",
      "2: ──H──RY(0.21)──╭●─╰X──────────RY(0.01)──╭●─╰X─────────RY(-0.01)─╭●─╰X──────────RY(0.01)──╭●\n",
      "3: ──H──RY(-0.84)─╰X─╭●──────────RY(-0.01)─╰X─╭●─────────RY(0.01)──╰X─╭●──────────RY(0.01)──╰X\n",
      "4: ──H──RY(-0.98)────╰X──────────RY(-0.00)────╰X─────────RY(-0.01)────╰X──────────RY(-0.01)───\n",
      "\n",
      "───RY(0.01)────────────╭●──RY(0.01)────────────╭●──RY(0.01)────────────┤  <Z>\n",
      "──╭●─────────RY(-0.00)─╰X─╭●─────────RY(-0.01)─╰X─╭●─────────RY(0.02)──┤  <Z>\n",
      "──╰X─────────RY(-0.00)─╭●─╰X─────────RY(0.01)──╭●─╰X─────────RY(-0.01)─┤  <Z>\n",
      "──╭●─────────RY(0.00)──╰X─╭●─────────RY(0.01)──╰X─╭●─────────RY(-0.00)─┤  <Z>\n",
      "──╰X─────────RY(0.02)─────╰X─────────RY(-0.01)────╰X─────────RY(-0.00)─┤  <Z>\n"
     ]
    }
   ],
   "source": [
    "# Example visualization codes with nonsense weights\n",
    "# Helps to see what the quantum circuit looks like\n",
    "q_params_print = nn.Parameter(q_delta * torch.randn(circ_repeats * qubit_count))\n",
    "input_test = torch.randn(1,512)\n",
    "pre_net_test = nn.Linear(512, qubit_count)\n",
    "pre_out_test = pre_net_test(input_test)\n",
    "q_in_test = torch.tanh(pre_out_test) * np.pi / 2.0\n",
    "for elem in q_in_test:\n",
    "    print(qml.draw(quantum_circuit)(elem, q_params_print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916720a3-5e54-4bee-b428-7be9e3252bdc",
   "metadata": {},
   "source": [
    "## HQNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94c56560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HQNN Model Class\n",
    "class HybridQuantumNet(nn.Module):\n",
    "    # Initialize layers\n",
    "    def __init__(self, num_feature):\n",
    "        super().__init__()\n",
    "        # Layers before quantum circuit\n",
    "        self.layer_1 = nn.Linear(num_feature, 512)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.layer_2 = nn.Linear(512, 128)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.layer_3 = nn.Linear(128, 64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.layer_4 = nn.Linear(64, qubit_count)\n",
    "        # Quantum Circuit params\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(circ_repeats * qubit_count))\n",
    "        # Layer(s) after quantum circuit\n",
    "        # self.post_quant = torch.nn.Softmax(dim=qubit_count)\n",
    "        # self.post_quant = nn.Linear(qubit_count, 5)\n",
    "        # Misc\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    # Forward pass procedure\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third layer\n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Map classical 64 output to qubit_count output\n",
    "        pre_out = self.layer_4(x)\n",
    "        \n",
    "        # Convert to radians for use as RY rotation gate params\n",
    "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
    "        # Apply quantum circuit set to run on GPU\n",
    "        q_out = torch.Tensor(0, qubit_count)\n",
    "        q_out = q_out.to(device)\n",
    "        for elem in q_in:\n",
    "            q_out_elem = quantum_circuit(elem, self.q_params)\n",
    "            q_out_elem = torch.stack(list(q_out_elem), dim=0)\n",
    "            q_out_elem = q_out_elem.float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "        # Map quantum circuit output using softmax to classes\n",
    "        # return self.post_quant(q_out)\n",
    "        return q_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "036f4bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridQuantumNet(\n",
      "  (layer_1): Linear(in_features=20264, out_features=512, bias=True)\n",
      "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_4): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model_hybrid = HybridQuantumNet(num_feature=len(df_x.columns))\n",
    "# Make sure set to use GPU\n",
    "model_hybrid = model_hybrid.to(device)\n",
    "# Show model architecture summary\n",
    "print(model_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ef09c-cd06-41b8-bd44-35bded59fab8",
   "metadata": {},
   "source": [
    "## Train HQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad1b52f9-0e31-4ea4-8edb-41272c82a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.0004  # Initial learning rate\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "gamma_lr_scheduler = 0.1  # Learning rate decay param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24238e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss, optimizer, and learning rate decay manager\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer_hybrid = optim.Adam(model_hybrid.parameters(), lr=step)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "    optimizer_hybrid, step_size=10, gamma=gamma_lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "035d2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model(s)\n",
    "def train(model, loss_func, optimizer, scheduler, num_epochs):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100000.0\n",
    "    best_loss_train = 100000.0\n",
    "    print('Training started:')\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                # Set model to train mode\n",
    "                model.train()\n",
    "            else:\n",
    "                # Set model to eval mode\n",
    "                model.eval()\n",
    "            current_loss = 0.0\n",
    "            n_batches = dataset_sizes[phase] // batch_size\n",
    "            iter = 0\n",
    "            for X, Y in dataloaders[phase]:\n",
    "                batch_len = len(X)\n",
    "                X = X.to(device)\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                # If in train mode, get loss, step optimizer\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(X)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = loss_func(outputs, Y)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # Print iteration results\n",
    "                current_loss += loss.item() * batch_len\n",
    "                print('Phase: {} Epoch: {}/{} Iter: {}/{}'.format(phase, epoch+1, num_epochs, iter+1, n_batches+1),\n",
    "                    end=\"\\r\",\n",
    "                    flush=True)\n",
    "                iter += 1\n",
    "\n",
    "            # Get epoch stats and print\n",
    "            epoch_loss = current_loss / dataset_sizes[phase]\n",
    "            print('Phase: {} Epoch: {}/{} Loss: {:.4f}        '.format(\n",
    "                    'train' if phase == 'train' else 'validation  ',\n",
    "                    epoch + 1,\n",
    "                    num_epochs,\n",
    "                    epoch_loss,\n",
    "                )\n",
    "            )\n",
    "            # Update best vars and make copy of best weights\n",
    "            # if phase == \"validation\":\n",
    "            #     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"validation\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
    "                best_loss_train = epoch_loss\n",
    "            # Decay learning rate\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "    # Print final results\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Best test loss: {:.4f}'.format(best_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ade40fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b4d9e985454af9a71331c0984c1241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train Epoch: 1/50 Loss: 0.5953        \n",
      "Phase: validation   Epoch: 1/50 Loss: 0.5217        \n",
      "Phase: train Epoch: 2/50 Loss: 0.5982        \n",
      "Phase: validation   Epoch: 2/50 Loss: 0.5225        \n",
      "Phase: train Epoch: 3/50 Loss: 0.6104        \n",
      "Phase: validation   Epoch: 3/50 Loss: 0.5232        \n",
      "Phase: train Epoch: 4/50 Loss: 0.5845        \n",
      "Phase: validation   Epoch: 4/50 Loss: 0.5239        \n",
      "Phase: train Epoch: 5/50 Loss: 0.5742        \n",
      "Phase: validation   Epoch: 5/50 Loss: 0.5221        \n",
      "Phase: train Epoch: 6/50 Loss: 0.5702        \n",
      "Phase: validation   Epoch: 6/50 Loss: 0.5224        \n",
      "Phase: train Epoch: 7/50 Loss: 0.5702        \n",
      "Phase: validation   Epoch: 7/50 Loss: 0.5251        \n",
      "Phase: train Epoch: 8/50 Loss: 0.5833        \n",
      "Phase: validation   Epoch: 8/50 Loss: 0.5206        \n",
      "Phase: train Epoch: 9/50 Loss: 0.5769        \n",
      "Phase: validation   Epoch: 9/50 Loss: 0.5223        \n",
      "Phase: train Epoch: 10/50 Loss: 0.5625        \n",
      "Phase: validation   Epoch: 10/50 Loss: 0.5228        \n",
      "Phase: train Epoch: 11/50 Iter: 47/65\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_hybrid \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_hybrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_hybrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# If in train mode, get loss, step optimizer\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, Y)\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 51\u001b[0m, in \u001b[0;36mHybridQuantumNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m q_out \u001b[38;5;241m=\u001b[39m q_out\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m q_in:\n\u001b[0;32m---> 51\u001b[0m     q_out_elem \u001b[38;5;241m=\u001b[39m \u001b[43mquantum_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     q_out_elem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mlist\u001b[39m(q_out_elem), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     53\u001b[0m     q_out_elem \u001b[38;5;241m=\u001b[39m q_out_elem\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/pennylane/qnode.py:1027\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         full_transform_program\u001b[38;5;241m.\u001b[39m_set_all_argnums(\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m, args, kwargs, argnums\n\u001b[1;32m   1024\u001b[0m         )  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m-> 1027\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/pennylane/interfaces/execution.py:616\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, gradient_fn, interface, transform_program, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[0;32m--> 616\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43minner_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(results)\n\u001b[1;32m    619\u001b[0m _grad_on_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/pennylane/interfaces/execution.py:249\u001b[0m, in \u001b[0;36m_make_inner_execute.<locals>.inner_execute\u001b[0;34m(tapes, **_)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_only:\n\u001b[1;32m    248\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(qml\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mconvert_to_numpy_parameters(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tapes)\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_device_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/pennylane/interfaces/execution.py:332\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m repeated \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tapes):\n\u001b[0;32m--> 332\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hashes\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;66;03m# Tape already exists within ``tapes``. Determine the\u001b[39;00m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;66;03m# index of the first occurrence of the tape, store this,\u001b[39;00m\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;66;03m# and continue to the next iteration.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m         idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(hashes\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;28mlist\u001b[39m(hashes\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mindex(h)]\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/pennylane/tape/qscript.py:237\u001b[0m, in \u001b[0;36mQuantumScript.hash\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"int: returns an integer hash uniquely representing the quantum script\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m fingerprint \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 237\u001b[0m fingerprint\u001b[38;5;241m.\u001b[39mextend(op\u001b[38;5;241m.\u001b[39mhash \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperations)\n\u001b[1;32m    238\u001b[0m fingerprint\u001b[38;5;241m.\u001b[39mextend(m\u001b[38;5;241m.\u001b[39mhash \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeasurements)\n\u001b[1;32m    239\u001b[0m fingerprint\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_params)\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/pennylane/tape/qscript.py:237\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"int: returns an integer hash uniquely representing the quantum script\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m fingerprint \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 237\u001b[0m fingerprint\u001b[38;5;241m.\u001b[39mextend(\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperations)\n\u001b[1;32m    238\u001b[0m fingerprint\u001b[38;5;241m.\u001b[39mextend(m\u001b[38;5;241m.\u001b[39mhash \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeasurements)\n\u001b[1;32m    239\u001b[0m fingerprint\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_params)\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/pennylane/operation.py:718\u001b[0m, in \u001b[0;36mOperator.hash\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    712\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"int: Integer hash that uniquely represents the operator.\"\"\"\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\n\u001b[1;32m    714\u001b[0m         (\n\u001b[1;32m    715\u001b[0m             \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    716\u001b[0m             \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwires\u001b[38;5;241m.\u001b[39mtolist()),\n\u001b[1;32m    717\u001b[0m             \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters\u001b[38;5;241m.\u001b[39mvalues()),\n\u001b[0;32m--> 718\u001b[0m             \u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    719\u001b[0m         )\n\u001b[1;32m    720\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/pennylane/operation.py:383\u001b[0m, in \u001b[0;36m_process_data\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_data\u001b[39m(op):\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# Use qml.math.real to take the real part. We may get complex inputs for\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# example when differentiating holomorphic functions with JAX: a complex\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;66;03m# valued QNode (one that returns qml.state) requires complex typed inputs.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhaseShift\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRot\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 383\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m([qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mround(qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreal(d) \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi), \u001b[38;5;241m10\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m op\u001b[38;5;241m.\u001b[39mdata])\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRot\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m([qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mround(qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreal(d) \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi), \u001b[38;5;241m10\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m op\u001b[38;5;241m.\u001b[39mdata])\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/opt/conda/envs/ml4fg/lib/python3.11/site-packages/torch/_tensor_str.py:137\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_hybrid = train(model_hybrid, loss_func, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e7672ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights locally in case of GCP crash\n",
    "torch.save(model_hybrid.state_dict(), './weights/hybrid_50epochs.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd073d9f-88a8-43c1-a147-1a6ca3baef3c",
   "metadata": {},
   "source": [
    "## Test HQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef720e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in saved weights, put in model, and than set to eval mode\n",
    "best_weights = torch.load('./weights/hybrid_50epochs.pt')\n",
    "model_hybrid.load_state_dict(best_weights)\n",
    "model_hybrid.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59a5612b-2ac3-453c-bd0b-2d8baf870fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d25292419f4e6986f7a1c539abd7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function to test model\n",
    "def test(model, device, test_loader):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Send the data and target to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            preds.append(enc.inverse_transform(output)[0])\n",
    "            targets.append(enc.inverse_transform(target)[0])\n",
    "    return preds, targets\n",
    "\n",
    "preds, targets = test(model_hybrid, 'cpu', dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6f576c3-ffc2-498f-85a0-f1e5cf62d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.78      1.00      0.88        60\n",
      "        COAD       1.00      1.00      1.00        16\n",
      "        KIRC       1.00      0.43      0.60        30\n",
      "        LUAD       1.00      1.00      1.00        28\n",
      "        PRAD       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           0.89       161\n",
      "   macro avg       0.96      0.89      0.90       161\n",
      "weighted avg       0.92      0.89      0.88       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=targets, y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e278d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
